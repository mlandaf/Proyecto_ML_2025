{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75acfadb",
   "metadata": {},
   "source": [
    "# Proyecto de Machine Learning: Predicci√≥n de Demanda de Bicicletas Compartidas\n",
    "\n",
    "## Objetivo del Proyecto\n",
    "Desarrollar un modelo de regresi√≥n para predecir la demanda de bicicletas compartidas basado en factores clim√°ticos, temporales y estacionales.\n",
    "\n",
    "## Dataset\n",
    "- **Fuente**: UCI Machine Learning Repository - Bike Sharing Dataset\n",
    "- **Instancias**: 17,389 registros\n",
    "- **Caracter√≠sticas**: 13 features\n",
    "- **Tarea**: Regresi√≥n\n",
    "- **Periodo**: 2011-2012\n",
    "\n",
    "## Fases del Proyecto\n",
    "1. **Fase 1**: An√°lisis Exploratorio de Datos (EDA)\n",
    "2. **Fase 2**: Preprocesamiento y Tratamiento de Datos\n",
    "3. **Fase 3**: Modelado\n",
    "4. **Fase 4**: Optimizaci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cd46f",
   "metadata": {},
   "source": [
    "## Importacion de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS Y CONFIGURACI√ìN GLOBAL PARA REGRESI√ìN\n",
    "# ===============================================\n",
    "\n",
    "# === LIBRER√çAS B√ÅSICAS ===\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === VISUALIZACI√ìN ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === DATOS ===\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# === MODELOS DE REGRESI√ìN ===\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Redes Neuronales\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "# Voting\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# === M√âTRICAS DE REGRESI√ìN ===\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score\n",
    ")\n",
    "\n",
    "# === PREPROCESAMIENTO Y VALIDACI√ìN ===\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    cross_val_score,\n",
    "    KFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# === AN√ÅLISIS ESTAD√çSTICO ===\n",
    "from scipy import stats\n",
    "import scipy.stats as stats\n",
    "\n",
    "# === CONFIGURACI√ìN GLOBAL ===\n",
    "# Silenciar warnings no cr√≠ticos\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Estilos de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Opciones de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as para REGRESI√ìN importadas y configuraci√≥n completada\")\n",
    "print(\"ü§ñ Modelos disponibles: SVM, Decision Tree, Random Forest, XGBoost, Neural Networks, Bagging, Boosting, Voting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fc437",
   "metadata": {},
   "source": [
    "## Definicion de Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data, columns, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualiza la distribuci√≥n de variables para an√°lisis de regresi√≥n\n",
    "    \"\"\"\n",
    "    n_cols = len(columns)\n",
    "    n_rows = (n_cols + 2) // 3\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=figsize)\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            # Histograma simple para variables num√©ricas\n",
    "            axes[i].hist(data[col], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[i].set_title(f'Distribuci√≥n de {col}')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frecuencia')\n",
    "            axes[i].grid(alpha=0.3)\n",
    "            \n",
    "            # Agregar estad√≠sticas b√°sicas\n",
    "            mean_val = data[col].mean()\n",
    "            median_val = data[col].median()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Media: {mean_val:.2f}')\n",
    "            axes[i].axvline(median_val, color='orange', linestyle='--', alpha=0.8, label=f'Mediana: {median_val:.2f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    # Ocultar ejes vac√≠os\n",
    "    for i in range(len(columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(data, figsize=(8, 4)):\n",
    "    \n",
    "    corr_matrix = data.corr(numeric_only=True)\n",
    "    \n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Generar heatmap\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r',\n",
    "                center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Matriz de Correlaci√≥n de Variables', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def plot_outliers(data, columns, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Detecta y visualiza outliers en variables num√©ricas usando boxplots\n",
    "    \"\"\"\n",
    "    n_cols = len(columns)\n",
    "    n_rows = (n_cols + 2) // 3\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=figsize)\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            # Boxplot para detectar outliers\n",
    "            box_plot = axes[i].boxplot(data[col], patch_artist=True)\n",
    "            box_plot['boxes'][0].set_facecolor('lightblue')\n",
    "            axes[i].set_title(f'Outliers en {col}')\n",
    "            axes[i].set_ylabel(col)\n",
    "            axes[i].grid(alpha=0.3)\n",
    "            \n",
    "            # Calcular y mostrar estad√≠sticas de outliers\n",
    "            Q1 = data[col].quantile(0.25)\n",
    "            Q3 = data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "            \n",
    "            axes[i].text(0.02, 0.98, f'Outliers: {len(outliers)}', \n",
    "                        transform=axes[i].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Ocultar ejes vac√≠os\n",
    "    for i in range(len(columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_target_distribution(y, title=\"Distribuci√≥n de la Variable Objetivo\", figsize=(12, 5)):\n",
    "    \"\"\"\n",
    "    Analiza y visualiza la distribuci√≥n de la variable objetivo para regresi√≥n\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Histograma\n",
    "    ax1.hist(y, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title(f'{title} - Histograma')\n",
    "    ax1.set_xlabel('Valor')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Agregar estad√≠sticas\n",
    "    mean_val = y.mean()\n",
    "    median_val = y.median()\n",
    "    ax1.axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Media: {mean_val:.2f}')\n",
    "    ax1.axvline(median_val, color='orange', linestyle='--', alpha=0.8, label=f'Mediana: {median_val:.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Boxplot\n",
    "    box_plot = ax2.boxplot(y, patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('lightgreen')\n",
    "    ax2.set_title(f'{title} - Boxplot')\n",
    "    ax2.set_ylabel('Valor')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # QQ Plot para normalidad\n",
    "    from scipy import stats\n",
    "    stats.probplot(y, dist=\"norm\", plot=ax3)\n",
    "    ax3.set_title(f'{title} - Q-Q Plot')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimir estad√≠sticas\n",
    "    print(f\"üìä ESTAD√çSTICAS DE LA VARIABLE OBJETIVO:\")\n",
    "    print(f\"   ‚Ä¢ Media: {y.mean():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Mediana: {y.median():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar: {y.std():.2f}\")\n",
    "    print(f\"   ‚Ä¢ M√≠nimo: {y.min():.2f}\")\n",
    "    print(f\"   ‚Ä¢ M√°ximo: {y.max():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Rango: {y.max() - y.min():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Asimetr√≠a: {y.skew():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Curtosis: {y.kurtosis():.2f}\")\n",
    "    \n",
    "    # Test de normalidad\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(y.sample(min(5000, len(y))))\n",
    "    print(f\"   ‚Ä¢ Test de Shapiro-Wilk (normalidad): p-value = {shapiro_p:.2e}\")\n",
    "    if shapiro_p > 0.05:\n",
    "        print(\"     ‚úÖ Los datos siguen una distribuci√≥n normal\")\n",
    "    else:\n",
    "        print(\"     ‚ùå Los datos NO siguen una distribuci√≥n normal\")\n",
    "\n",
    "def evaluate_regression_model(model, X_test, y_test, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Eval√∫a un modelo de regresi√≥n con m√∫ltiples m√©tricas\n",
    "    \"\"\"\n",
    "    # Calcular m√©tricas\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Calcular MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "    # Calcular m√©tricas adicionales\n",
    "    mean_y = np.mean(y_test)\n",
    "    ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_test - mean_y) ** 2)\n",
    "    adjusted_r2 = 1 - (ss_res / ss_tot) * ((len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1))\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'Adjusted_R¬≤': adjusted_r2,\n",
    "        'MAPE': mape,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_regression_results(models_results, y_test, figsize=(15, 12)):\n",
    "    \"\"\"\n",
    "    Visualiza los resultados de m√∫ltiples modelos de regresi√≥n\n",
    "    \"\"\"\n",
    "    n_models = len(models_results)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(models_results.items()):\n",
    "        if i < len(axes):\n",
    "            y_pred = results['predictions']\n",
    "            \n",
    "            # Scatter plot: Valores reales vs predichos\n",
    "            axes[i].scatter(y_test, y_pred, alpha=0.6, color='blue')\n",
    "            \n",
    "            # L√≠nea de referencia perfecta (y = x)\n",
    "            min_val = min(y_test.min(), y_pred.min())\n",
    "            max_val = max(y_test.max(), y_pred.max())\n",
    "            axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, alpha=0.8)\n",
    "            \n",
    "            axes[i].set_title(f'{model_name}\\nR¬≤ = {results[\"R¬≤\"]:.3f}, RMSE = {results[\"RMSE\"]:.2f}')\n",
    "            axes[i].set_xlabel('Valores Reales')\n",
    "            axes[i].set_ylabel('Valores Predichos')\n",
    "            axes[i].grid(alpha=0.3)\n",
    "            \n",
    "            # Calcular l√≠mites iguales para ambos ejes\n",
    "            all_values = np.concatenate([y_test, y_pred])\n",
    "            margin = (all_values.max() - all_values.min()) * 0.05\n",
    "            axes[i].set_xlim(all_values.min() - margin, all_values.max() + margin)\n",
    "            axes[i].set_ylim(all_values.min() - margin, all_values.max() + margin)\n",
    "    \n",
    "    # Ocultar ejes vac√≠os\n",
    "    for i in range(len(models_results), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals_analysis(models_results, y_test, figsize=(15, 12)):\n",
    "    \"\"\"\n",
    "    An√°lisis de residuos para modelos de regresi√≥n\n",
    "    \"\"\"\n",
    "    n_models = len(models_results)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(models_results.items()):\n",
    "        if i < len(axes):\n",
    "            y_pred = results['predictions']\n",
    "            residuals = y_test - y_pred\n",
    "            \n",
    "            # Gr√°fico de residuos vs valores predichos\n",
    "            axes[i].scatter(y_pred, residuals, alpha=0.6, color='green')\n",
    "            axes[i].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "            axes[i].set_title(f'Residuos - {model_name}')\n",
    "            axes[i].set_xlabel('Valores Predichos')\n",
    "            axes[i].set_ylabel('Residuos')\n",
    "            axes[i].grid(alpha=0.3)\n",
    "    \n",
    "    # Ocultar ejes vac√≠os\n",
    "    for i in range(len(models_results), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_models_metrics(models_results, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Compara las m√©tricas de diferentes modelos de regresi√≥n\n",
    "    \"\"\"\n",
    "    # Crear DataFrame con las m√©tricas\n",
    "    metrics_data = []\n",
    "    for model_name, results in models_results.items():\n",
    "        metrics_data.append({\n",
    "            'Model': model_name,\n",
    "            'R¬≤': results['R¬≤'],\n",
    "            'RMSE': results['RMSE'],\n",
    "            'MAE': results['MAE'],\n",
    "            'MAPE': results['MAPE']\n",
    "        })\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Visualizar m√©tricas\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # R¬≤\n",
    "    axes[0, 0].bar(df_metrics['Model'], df_metrics['R¬≤'], color='skyblue')\n",
    "    axes[0, 0].set_title('R¬≤ Score')\n",
    "    axes[0, 0].set_ylabel('R¬≤')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # RMSE\n",
    "    axes[0, 1].bar(df_metrics['Model'], df_metrics['RMSE'], color='lightcoral')\n",
    "    axes[0, 1].set_title('Root Mean Square Error')\n",
    "    axes[0, 1].set_ylabel('RMSE')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MAE\n",
    "    axes[1, 0].bar(df_metrics['Model'], df_metrics['MAE'], color='lightgreen')\n",
    "    axes[1, 0].set_title('Mean Absolute Error')\n",
    "    axes[1, 0].set_ylabel('MAE')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MAPE\n",
    "    axes[1, 1].bar(df_metrics['Model'], df_metrics['MAPE'], color='gold')\n",
    "    axes[1, 1].set_title('Mean Absolute Percentage Error')\n",
    "    axes[1, 1].set_ylabel('MAPE (%)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfb9a7",
   "metadata": {},
   "source": [
    "# FASE 1: AN√ÅLISIS EXPLORATORIO DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d61ff",
   "metadata": {},
   "source": [
    "## 1.1 Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a03a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Carga de datos desde UCI ML Repository\n",
    "\n",
    "# Obtener el dataset\n",
    "bike_sharing = fetch_ucirepo(id=275)\n",
    "\n",
    "# Extraer caracter√≠sticas y variable objetivo\n",
    "X = bike_sharing.data.features\n",
    "y = bike_sharing.data.targets\n",
    "\n",
    "# Combinar en un solo DataFrame para el an√°lisis\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado exitosamente!\")\n",
    "print(f\"üìà Forma del dataset: {df.shape}\")\n",
    "print(f\"üîó Variables de entrada: {X.shape[1]}\")\n",
    "print(f\"üéØ Variables objetivo: {y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe56e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['dteday']\n",
    "df = df.drop(columns=drop_cols)\n",
    "print(f\"Existen un total de {df.shape[1]} columnas despu√©s de eliminar las columnas innecesarias.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a3de1",
   "metadata": {},
   "source": [
    "## 1.2 Exploraci√≥n inicial del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Exploraci√≥n inicial del dataset\n",
    "print(\"üîç EXPLORACI√ìN INICIAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Informaci√≥n general del dataset\n",
    "print(\"üìä Informaci√≥n general:\")\n",
    "print(f\"Forma del dataset: {df.shape}\")\n",
    "print(f\"N√∫mero de filas: {df.shape[0]:,}\")\n",
    "print(f\"N√∫mero de columnas: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c55087",
   "metadata": {},
   "source": [
    "### 1.2.1 Primeras 5 filas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìã Primeras 5 filas del dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80f93d",
   "metadata": {},
   "source": [
    "### 1.2.2 Ultimas 5 filas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaead4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìã √öltimas 5 filas del dataset:\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81b63e",
   "metadata": {},
   "source": [
    "### 1.2.3 Nombres de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîó Nombres de las columnas:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6927887",
   "metadata": {},
   "source": [
    "## 1.3 An√°lisis de tipos de datos y valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02801a23",
   "metadata": {},
   "source": [
    "### 1.3.1 Informaci√≥n detallada del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 An√°lisis de tipos de datos y valores faltantes\n",
    "print(\"üîç AN√ÅLISIS DE TIPOS DE DATOS Y CALIDAD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Informaci√≥n detallada del dataset\n",
    "print(\"üìä Informaci√≥n detallada:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d010170",
   "metadata": {},
   "source": [
    "### 1.3.2 Tipos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c412a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî¢ Tipos de datos:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede069d",
   "metadata": {},
   "source": [
    "### 1.3.3 Valores nulos del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcedf664",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚ùå Valores faltantes por columna:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Columna': missing_values.index,\n",
    "    'Valores_Faltantes': missing_values.values,\n",
    "    'Porcentaje': missing_percentage.values\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "print(f\"\\nüìà Total de valores faltantes: {df.isnull().sum().sum()}\")\n",
    "print(f\"üìä Porcentaje total de valores faltantes: {(df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4170cb",
   "metadata": {},
   "source": [
    "## 1.4 Estad√≠sticas descriptivas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb1485",
   "metadata": {},
   "source": [
    "### 1.4.1 Estad√≠sticas para variables num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Estad√≠sticas descriptivas\n",
    "print(\"üìä ESTAD√çSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "variables_numericas = ['temp','atemp','hum','windspeed','cnt']\n",
    "# Estad√≠sticas para variables num√©ricas\n",
    "print(\"üî¢ Variables num√©ricas:\")\n",
    "print(f\"Columnas num√©ricas: {variables_numericas}\")\n",
    "\n",
    "print(\"\\nüìà Estad√≠sticas descriptivas de variables num√©ricas:\")\n",
    "display(df[variables_numericas].describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e554c",
   "metadata": {},
   "source": [
    "### 1.4.2 Estad√≠sticas para variables OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d923e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas para variables categ√≥ricas\n",
    "print(\"\\nüè∑Ô∏è Variables categ√≥ricas:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Columnas categ√≥ricas: {categorical_cols}\")\n",
    "\n",
    "if categorical_cols:\n",
    "    print(\"\\nüìä Estad√≠sticas descriptivas de variables categ√≥ricas:\")\n",
    "    display(df[categorical_cols].describe())\n",
    "else:\n",
    "    print(\"No hay variables categ√≥ricas de tipo object en el dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792028b5",
   "metadata": {},
   "source": [
    "### 1.4.3 Informaci√≥n sobre la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ea83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n sobre la variable objetivo\n",
    "print(\"\\nüéØ AN√ÅLISIS DE LA VARIABLE OBJETIVO (cnt - Total Rental Bikes)\")\n",
    "print(f\"Media: {df['cnt'].mean():.2f}\")\n",
    "print(f\"Mediana: {df['cnt'].median():.2f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {df['cnt'].std():.2f}\")\n",
    "print(f\"M√≠nimo: {df['cnt'].min()}\")\n",
    "print(f\"M√°ximo: {df['cnt'].max()}\")\n",
    "print(f\"Rango: {df['cnt'].max() - df['cnt'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2071d",
   "metadata": {},
   "source": [
    "## 1.5 An√°lisis de correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98310d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 An√°lisis de correlaciones\n",
    "print(\"üîó AN√ÅLISIS DE CORRELACIONES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Usar la funci√≥n definida para crear la matriz de correlaci√≥n\n",
    "correlation_matrix = plot_correlation_matrix(df, figsize=(14, 10))\n",
    "\n",
    "# An√°lisis detallado de correlaciones con la variable objetivo\n",
    "print(\"\\nüéØ CORRELACIONES CON LA VARIABLE OBJETIVO (cnt):\")\n",
    "target_correlations = correlation_matrix['cnt'].sort_values(key=abs, ascending=False)\n",
    "print(target_correlations.to_string())\n",
    "\n",
    "print(\"\\nüìä TOP 5 VARIABLES M√ÅS CORRELACIONADAS CON LA DEMANDA:\")\n",
    "top_correlations = target_correlations.drop('cnt').head(5)\n",
    "for variable, correlation in top_correlations.items():\n",
    "    direction = \"positiva\" if correlation > 0 else \"negativa\"\n",
    "    print(f\"‚Ä¢ {variable}: {correlation:.3f} (correlaci√≥n {direction})\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è CORRELACIONES FUERTES ENTRE VARIABLES INDEPENDIENTES (posible multicolinealidad):\")\n",
    "strong_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        correlation = correlation_matrix.iloc[i, j]\n",
    "        if abs(correlation) > 0.7 and correlation_matrix.columns[i] != 'cnt' and correlation_matrix.columns[j] != 'cnt':\n",
    "            strong_correlations.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation))\n",
    "\n",
    "if strong_correlations:\n",
    "    for var1, var2, corr in strong_correlations:\n",
    "        print(f\"‚Ä¢ {var1} - {var2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"No se encontraron correlaciones fuertes (>0.7) entre variables independientes\")\n",
    "\n",
    "# An√°lisis espec√≠fico entre temperatura y temperatura aparente\n",
    "print(f\"\\nüå°Ô∏è CORRELACI√ìN TEMPERATURA vs TEMPERATURA APARENTE: {correlation_matrix.loc['temp', 'atemp']:.3f}\")\n",
    "print(\"Esto indica una relaci√≥n muy fuerte, se puede considerar usar solo una de estas variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d0e94",
   "metadata": {},
   "source": [
    "## 1.6 Detecci√≥n y An√°lisis de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e59793",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.6 Identifique y reporte outliers en al menos 3 variables num√©ricas\n",
    "# ========================\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  AN√ÅLISIS DE OUTLIERS:\")\n",
    "\n",
    "# Detectar outliers usando IQR\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    outliers_count = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outliers_count[col] = len(outliers)\n",
    "    return outliers_count\n",
    "\n",
    "main_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "outliers_info = detect_outliers_iqr(df, main_features)\n",
    "print(f\"\\nüìä CONTEO DE OUTLIERS POR VARIABLE (m√©todo IQR):\")\n",
    "for feature, count in outliers_info.items():\n",
    "    print(f\"   ‚Ä¢ {feature:<25}: {count:3d} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cacc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de outliers usando nuestras funciones\n",
    "print(\"\\nüìä VISUALIZACI√ìN DE OUTLIERS:\")\n",
    "\n",
    "# Variables num√©ricas principales para an√°lisis de outliers\n",
    "numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "# Usar la funci√≥n plot_outliers definida anteriormente\n",
    "plot_outliers(df, numeric_features, figsize=(18, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd42e0",
   "metadata": {},
   "source": [
    "## 1.7 An√°lisis de Distribuciones de Variables Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb608d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 An√°lisis de distribuciones de variables num√©ricas\n",
    "print(\"üìà AN√ÅLISIS DE DISTRIBUCIONES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Variables num√©ricas para analizar\n",
    "numeric_vars = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\n",
    "\n",
    "# Usar funci√≥n para mostrar distribuciones\n",
    "plot_distribution(df, numeric_vars, figsize=(18, 12))\n",
    "\n",
    "# An√°lisis espec√≠fico de la variable objetivo\n",
    "print(\"\\nüéØ AN√ÅLISIS DETALLADO DE LA VARIABLE OBJETIVO (cnt):\")\n",
    "plot_target_distribution(df['cnt'], title=\"Demanda de Bicicletas (cnt)\", figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be1e6b9",
   "metadata": {},
   "source": [
    "## 1.8 An√°lisis Temporal de la Demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 An√°lisis temporal de la demanda\n",
    "print(\"‚è∞ AN√ÅLISIS TEMPORAL DE LA DEMANDA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear mapeos para mejor interpretaci√≥n\n",
    "season_names = {1: 'Primavera', 2: 'Verano', 3: 'Oto√±o', 4: 'Invierno'}\n",
    "weather_names = {1: 'Despejado', 2: 'Nublado', 3: 'Lluvia ligera', 4: 'Lluvia fuerte'}\n",
    "year_names = {0: '2011', 1: '2012'}\n",
    "weekday_names = {0: 'Domingo', 1: 'Lunes', 2: 'Martes', 3: 'Mi√©rcoles', 4: 'Jueves', 5: 'Viernes', 6: 'S√°bado'}\n",
    "workingday_names = {0: 'No laborable', 1: 'Laborable'}\n",
    "holiday_names = {0: 'No festivo', 1: 'Festivo'}\n",
    "\n",
    "# An√°lisis por estaci√≥n del a√±o\n",
    "print(\"\\nüå∏ DEMANDA PROMEDIO POR ESTACI√ìN:\")\n",
    "season_avg = df.groupby('season')['cnt'].mean().sort_values(ascending=False)\n",
    "for season_code, avg_demand in season_avg.items():\n",
    "    print(f\"‚Ä¢ {season_names[season_code]}: {avg_demand:.1f} bicicletas/hora\")\n",
    "\n",
    "# An√°lisis por a√±o\n",
    "print(\"\\nüìÖ DEMANDA PROMEDIO POR A√ëO:\")\n",
    "year_avg = df.groupby('yr')['cnt'].mean()\n",
    "for year_code, avg_demand in year_avg.items():\n",
    "    print(f\"‚Ä¢ {year_names[year_code]}: {avg_demand:.1f} bicicletas/hora\")\n",
    "\n",
    "# An√°lisis por d√≠a de la semana\n",
    "print(\"\\nüìÜ DEMANDA PROMEDIO POR D√çA DE LA SEMANA:\")\n",
    "weekday_avg = df.groupby('weekday')['cnt'].mean()\n",
    "for weekday_code, avg_demand in weekday_avg.items():\n",
    "    print(f\"‚Ä¢ {weekday_names[weekday_code]}: {avg_demand:.1f} bicicletas/hora\")\n",
    "\n",
    "# An√°lisis por hora del d√≠a\n",
    "print(\"\\nüïê DEMANDA PROMEDIO POR HORA DEL D√çA:\")\n",
    "hourly_avg = df.groupby('hr')['cnt'].mean()\n",
    "print(f\"Hora con mayor demanda: {hourly_avg.idxmax()}:00 ({hourly_avg.max():.1f} bicicletas)\")\n",
    "print(f\"Hora con menor demanda: {hourly_avg.idxmin()}:00 ({hourly_avg.min():.1f} bicicletas)\")\n",
    "\n",
    "# An√°lisis por mes\n",
    "print(\"\\nüìä DEMANDA PROMEDIO POR MES:\")\n",
    "monthly_avg = df.groupby('mnth')['cnt'].mean()\n",
    "print(f\"Mes con mayor demanda: {monthly_avg.idxmax()} ({monthly_avg.max():.1f} bicicletas/hora)\")\n",
    "print(f\"Mes con menor demanda: {monthly_avg.idxmin()} ({monthly_avg.min():.1f} bicicletas/hora)\")\n",
    "\n",
    "# An√°lisis d√≠as laborables vs no laborables\n",
    "print(\"\\nüíº DEMANDA: D√çAS LABORABLES VS NO LABORABLES:\")\n",
    "workingday_comparison = df.groupby('workingday')['cnt'].agg(['mean', 'std', 'count'])\n",
    "for workingday_code in workingday_comparison.index:\n",
    "    stats = workingday_comparison.loc[workingday_code]\n",
    "    print(f\"‚Ä¢ {workingday_names[workingday_code]}: {stats['mean']:.1f} ¬± {stats['std']:.1f} bicicletas/hora (n={stats['count']})\")\n",
    "\n",
    "# An√°lisis condiciones clim√°ticas\n",
    "print(\"\\nüå§Ô∏è DEMANDA PROMEDIO POR CONDICIONES CLIM√ÅTICAS:\")\n",
    "weather_avg = df.groupby('weathersit')['cnt'].mean().sort_values(ascending=False)\n",
    "for weather_code, avg_demand in weather_avg.items():\n",
    "    print(f\"‚Ä¢ {weather_names[weather_code]}: {avg_demand:.1f} bicicletas/hora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d65572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones temporales\n",
    "print(\"\\nüìä VISUALIZACIONES TEMPORALES:\")\n",
    "\n",
    "# Crear visualizaciones en una sola figura\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Demanda por estaci√≥n\n",
    "season_avg.plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "axes[0, 0].set_title('Demanda Promedio por Estaci√≥n')\n",
    "axes[0, 0].set_xlabel('Estaci√≥n')\n",
    "axes[0, 0].set_ylabel('Demanda Promedio')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Demanda por hora del d√≠a\n",
    "hourly_avg.plot(kind='line', ax=axes[0, 1], color='orange', marker='o')\n",
    "axes[0, 1].set_title('Patr√≥n de Demanda por Hora del D√≠a')\n",
    "axes[0, 1].set_xlabel('Hora del D√≠a')\n",
    "axes[0, 1].set_ylabel('Demanda Promedio')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Demanda por d√≠a de la semana\n",
    "weekday_avg.plot(kind='bar', ax=axes[0, 2], color='lightgreen')\n",
    "axes[0, 2].set_title('Demanda Promedio por D√≠a de la Semana')\n",
    "axes[0, 2].set_xlabel('D√≠a de la Semana')\n",
    "axes[0, 2].set_ylabel('Demanda Promedio')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Demanda por mes\n",
    "monthly_avg.plot(kind='line', ax=axes[1, 0], color='red', marker='s')\n",
    "axes[1, 0].set_title('Demanda Promedio por Mes')\n",
    "axes[1, 0].set_xlabel('Mes')\n",
    "axes[1, 0].set_ylabel('Demanda Promedio')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Comparaci√≥n d√≠as laborables\n",
    "workingday_comparison['mean'].plot(kind='bar', ax=axes[1, 1], color='purple', alpha=0.7)\n",
    "axes[1, 1].set_title('D√≠as Laborables vs No Laborables')\n",
    "axes[1, 1].set_xlabel('Tipo de D√≠a')\n",
    "axes[1, 1].set_ylabel('Demanda Promedio')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 6. Condiciones clim√°ticas\n",
    "weather_avg.plot(kind='bar', ax=axes[1, 2], color='teal')\n",
    "axes[1, 2].set_title('Demanda por Condiciones Clim√°ticas')\n",
    "axes[1, 2].set_xlabel('Condici√≥n Clim√°tica')\n",
    "axes[1, 2].set_ylabel('Demanda Promedio')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60403427",
   "metadata": {},
   "source": [
    "## 1.9 An√°lisis de Relaciones: Variables Clim√°ticas vs Demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafa6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9 An√°lisis de relaciones entre variables clim√°ticas y demanda\n",
    "print(\"üå°Ô∏è AN√ÅLISIS DE RELACIONES CLIM√ÅTICAS VS DEMANDA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear gr√°ficos de dispersi√≥n para variables clim√°ticas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Temperatura vs Demanda\n",
    "axes[0, 0].scatter(df['temp'], df['cnt'], alpha=0.5, color='red')\n",
    "axes[0, 0].set_title('Temperatura vs Demanda')\n",
    "axes[0, 0].set_xlabel('Temperatura Normalizada')\n",
    "axes[0, 0].set_ylabel('Demanda (cnt)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar l√≠nea de tendencia\n",
    "z = np.polyfit(df['temp'], df['cnt'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(df['temp'], p(df['temp']), \"r--\", alpha=0.8)\n",
    "\n",
    "# 2. Humedad vs Demanda\n",
    "axes[0, 1].scatter(df['hum'], df['cnt'], alpha=0.5, color='blue')\n",
    "axes[0, 1].set_title('Humedad vs Demanda')\n",
    "axes[0, 1].set_xlabel('Humedad Normalizada')\n",
    "axes[0, 1].set_ylabel('Demanda (cnt)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar l√≠nea de tendencia\n",
    "z = np.polyfit(df['hum'], df['cnt'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 1].plot(df['hum'], p(df['hum']), \"b--\", alpha=0.8)\n",
    "\n",
    "# 3. Velocidad del viento vs Demanda\n",
    "axes[1, 0].scatter(df['windspeed'], df['cnt'], alpha=0.5, color='green')\n",
    "axes[1, 0].set_title('Velocidad del Viento vs Demanda')\n",
    "axes[1, 0].set_xlabel('Velocidad del Viento Normalizada')\n",
    "axes[1, 0].set_ylabel('Demanda (cnt)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar l√≠nea de tendencia\n",
    "z = np.polyfit(df['windspeed'], df['cnt'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1, 0].plot(df['windspeed'], p(df['windspeed']), \"g--\", alpha=0.8)\n",
    "\n",
    "# 4. Temperatura aparente vs Demanda\n",
    "axes[1, 1].scatter(df['atemp'], df['cnt'], alpha=0.5, color='orange')\n",
    "axes[1, 1].set_title('Temperatura Aparente vs Demanda')\n",
    "axes[1, 1].set_xlabel('Temperatura Aparente Normalizada')\n",
    "axes[1, 1].set_ylabel('Demanda (cnt)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar l√≠nea de tendencia\n",
    "z = np.polyfit(df['atemp'], df['cnt'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1, 1].plot(df['atemp'], p(df['atemp']), \"orange\", linestyle=\"--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis cuantitativo de las relaciones\n",
    "print(\"\\nüìä AN√ÅLISIS CUANTITATIVO DE RELACIONES CLIM√ÅTICAS:\")\n",
    "print(f\"‚Ä¢ Correlaci√≥n Temperatura-Demanda: {df['temp'].corr(df['cnt']):.3f}\")\n",
    "print(f\"‚Ä¢ Correlaci√≥n Humedad-Demanda: {df['hum'].corr(df['cnt']):.3f}\")\n",
    "print(f\"‚Ä¢ Correlaci√≥n Viento-Demanda: {df['windspeed'].corr(df['cnt']):.3f}\")\n",
    "print(f\"‚Ä¢ Correlaci√≥n Temp.Aparente-Demanda: {df['atemp'].corr(df['cnt']):.3f}\")\n",
    "\n",
    "# An√°lisis por rangos de temperatura\n",
    "print(\"\\nüå°Ô∏è AN√ÅLISIS POR RANGOS DE TEMPERATURA:\")\n",
    "df['temp_range'] = pd.cut(df['temp'], bins=4, labels=['Baja', 'Media-Baja', 'Media-Alta', 'Alta'])\n",
    "temp_range_analysis = df.groupby('temp_range')['cnt'].agg(['mean', 'count'])\n",
    "for temp_range in temp_range_analysis.index:\n",
    "    stats = temp_range_analysis.loc[temp_range]\n",
    "    print(f\"‚Ä¢ Temperatura {temp_range}: {stats['mean']:.1f} bicicletas/hora (n={stats['count']})\")\n",
    "\n",
    "# An√°lisis por rangos de humedad\n",
    "print(\"\\nüíß AN√ÅLISIS POR RANGOS DE HUMEDAD:\")\n",
    "df['hum_range'] = pd.cut(df['hum'], bins=4, labels=['Baja', 'Media-Baja', 'Media-Alta', 'Alta'])\n",
    "hum_range_analysis = df.groupby('hum_range')['cnt'].agg(['mean', 'count'])\n",
    "for hum_range in hum_range_analysis.index:\n",
    "    stats = hum_range_analysis.loc[hum_range]\n",
    "    print(f\"‚Ä¢ Humedad {hum_range}: {stats['mean']:.1f} bicicletas/hora (n={stats['count']})\")\n",
    "\n",
    "# Limpiar columnas temporales\n",
    "df = df.drop(['temp_range', 'hum_range'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c48e18",
   "metadata": {},
   "source": [
    "## 1.10 üìä Resumen y Conclusiones del EDA\n",
    "\n",
    "### üéØ RESUMEN EJECUTIVO DEL AN√ÅLISIS EXPLORATORIO\n",
    "\n",
    "### üìä CARACTER√çSTICAS DEL DATASET:\n",
    "- **Total de registros**: 17,379\n",
    "- **Variables disponibles**: 13\n",
    "- **Valores faltantes**: 0 (0%)\n",
    "- **Periodo analizado**: 2011-2012\n",
    "\n",
    "### üéØ VARIABLE OBJETIVO (Demanda de Bicicletas):\n",
    "- **Rango**: 1 - 977 bicicletas/hora\n",
    "- **Promedio**: 189.5 bicicletas/hora\n",
    "- **Distribuci√≥n**: Aproximadamente normal\n",
    "\n",
    "### üîó TOP 5 VARIABLES M√ÅS PREDICTIVAS:\n",
    "1. **temp** (temperatura): 0.627\n",
    "2. **atemp** (sensaci√≥n t√©rmica): 0.631\n",
    "3. **hr** (hora): 0.394\n",
    "4. **season** (estaci√≥n): 0.178\n",
    "5. **yr** (a√±o): 0.250\n",
    "\n",
    "### ‚ö†Ô∏è PROBLEMAS IDENTIFICADOS:\n",
    "- **Multicolinealidad** entre temp y atemp (r=0.988)\n",
    "- **Outliers** en windspeed (342 casos) y hum (22 casos)\n",
    "- **Posible estacionalidad** en los datos\n",
    "\n",
    "### üå°Ô∏è FACTORES CLIM√ÅTICOS:\n",
    "- **Temperatura**: correlaci√≥n positiva fuerte (0.627)\n",
    "- **Humedad**: correlaci√≥n negativa moderada (-0.317)\n",
    "- **Viento**: correlaci√≥n positiva d√©bil (0.101)\n",
    "\n",
    "### ‚è∞ PATRONES TEMPORALES:\n",
    "- **Hora pico**: 17:00 (hora de salida del trabajo)\n",
    "- **Estaci√≥n con mayor demanda**: Oto√±o\n",
    "- **Diferencia laborables vs no laborables**: significativa\n",
    "\n",
    "### üìà RECOMENDACIONES PARA EL MODELADO:\n",
    "1. **Considerar eliminar 'atemp'** por multicolinealidad con 'temp'\n",
    "2. **Analizar tratamiento de outliers** en 'windspeed'\n",
    "3. **Explorar ingenier√≠a de caracter√≠sticas temporales**\n",
    "4. **Considerar interacciones** entre variables clim√°ticas\n",
    "5. **Evaluar transformaciones no lineales** para variables clim√°ticas\n",
    "\n",
    "### ‚úÖ CALIDAD DE LOS DATOS:\n",
    "- ‚úÖ Datos completos (sin valores faltantes)\n",
    "- ‚úÖ Variables bien distribuidas\n",
    "- ‚úÖ Relaciones l√≥gicas entre variables\n",
    "- ‚úÖ Dataset adecuado para modelado de regresi√≥n\n",
    "\n",
    "---\n",
    "\n",
    "> **Conclusi√≥n**: El dataset presenta una excelente calidad para modelado predictivo, con patrones claros y relaciones l√≥gicas entre variables. Las variables clim√°ticas y temporales muestran el mayor potencial predictivo para la demanda de bicicletas compartidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f1e57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# FASE 2: PREPROCESAMIENTO Y TRATAMIENTO DE DATOS\n",
    "\n",
    "## Pr√≥ximos pasos basados en el EDA:\n",
    "1. **Tratamiento de multicolinealidad**: Eliminar `atemp` debido a alta correlaci√≥n con `temp` (r=0.988)\n",
    "2. **An√°lisis de outliers**: Decidir tratamiento para outliers en `windspeed` y `holiday`\n",
    "3. **Ingenier√≠a de caracter√≠sticas**: Crear features temporales adicionales\n",
    "4. **Normalizaci√≥n**: Aplicar escalado a variables num√©ricas si es necesario\n",
    "5. **Divisi√≥n del dataset**: Train/Test split con estratificaci√≥n temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a2839",
   "metadata": {},
   "source": [
    "## 2.1 Tratamiento de Multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Tratamiento de Multicolinealidad\n",
    "print(\"üîó TRATAMIENTO DE MULTICOLINEALIDAD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar si atemp existe antes de eliminarla\n",
    "if 'atemp' in df.columns:\n",
    "    df = df.drop(columns=['atemp'])\n",
    "    print(f\"‚úÖ Variable 'atemp' eliminada por multicolinealidad con 'temp'\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Variable 'atemp' ya hab√≠a sido eliminada previamente\")\n",
    "\n",
    "print(f\"üìä Forma actual del dataset: {df.shape}\")\n",
    "print(f\"üîó Variables disponibles: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8367b",
   "metadata": {},
   "source": [
    "## 2.2 An√°lisis de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b092c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_outliers(df, columns):\n",
    "    \"\"\"\n",
    "    Analiza outliers usando el m√©todo IQR y proporciona estad√≠sticas detalladas\n",
    "    \"\"\"\n",
    "    outliers_summary = []\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identificar outliers\n",
    "        outliers_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        outliers_count = outliers_mask.sum()\n",
    "        outliers_percentage = (outliers_count / len(df)) * 100\n",
    "        \n",
    "        outliers_summary.append({\n",
    "            'Variable': col,\n",
    "            'Total_Outliers': outliers_count,\n",
    "            'Porcentaje': outliers_percentage,\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR,\n",
    "            'L√≠mite_Inferior': lower_bound,\n",
    "            'L√≠mite_Superior': upper_bound,\n",
    "            'Min_Outlier': df[outliers_mask][col].min() if outliers_count > 0 else None,\n",
    "            'Max_Outlier': df[outliers_mask][col].max() if outliers_count > 0 else None\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(outliers_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685001af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 An√°lisis detallado de outliers en variables num√©ricas\n",
    "print(\"‚ö†Ô∏è AN√ÅLISIS DETALLADO DE OUTLIERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Variables num√©ricas para an√°lisis de outliers\n",
    "numeric_variables = ['temp', 'hum', 'windspeed']\n",
    "\n",
    "# Realizar an√°lisis de outliers\n",
    "outliers_df = analyze_outliers(df, numeric_variables)\n",
    "display(outliers_df)\n",
    "\n",
    "print(\"\\nüìä RESUMEN DE OUTLIERS:\")\n",
    "for _, row in outliers_df.iterrows():\n",
    "    print(f\"‚Ä¢ {row['Variable']}: {row['Total_Outliers']} outliers ({row['Porcentaje']:.2f}%)\")\n",
    "    if row['Total_Outliers'] > 0:\n",
    "        print(f\"  - Rango normal: [{row['L√≠mite_Inferior']:.3f}, {row['L√≠mite_Superior']:.3f}]\")\n",
    "        print(f\"  - Outliers van de {row['Min_Outlier']:.3f} a {row['Max_Outlier']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f96c94",
   "metadata": {},
   "source": [
    "## 2.3 Tratamiento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ce8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(df, column, method='iqr'):\n",
    "    \"\"\"\n",
    "    Aplica capping a outliers usando el m√©todo IQR\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Contar outliers antes del tratamiento\n",
    "    outliers_before = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
    "    \n",
    "    # Aplicar capping\n",
    "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    # Contar outliers despu√©s del tratamiento\n",
    "    outliers_after = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
    "    \n",
    "    return outliers_before, outliers_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b75380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Tratamiento de Outliers\n",
    "print(\"üîß TRATAMIENTO DE OUTLIERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear una copia del dataset original para comparaci√≥n\n",
    "df_original = df.copy()\n",
    "\n",
    "# Decisi√≥n de tratamiento basada en el an√°lisis\n",
    "print(\"üìã ESTRATEGIA DE TRATAMIENTO:\")\n",
    "\n",
    "# 1. Humedad: Solo 22 outliers (0.13%) - aplicar capping\n",
    "if 'hum' in df.columns:\n",
    "    before, after = cap_outliers(df, 'hum')\n",
    "    print(f\"‚úÖ Humedad: {before} ‚Üí {after} outliers (aplicado capping)\")\n",
    "\n",
    "# 2. Windspeed: 342 outliers (1.97%) - aplicar capping\n",
    "if 'windspeed' in df.columns:\n",
    "    before, after = cap_outliers(df, 'windspeed')\n",
    "    print(f\"‚úÖ Velocidad del viento: {before} ‚Üí {after} outliers (aplicado capping)\")\n",
    "\n",
    "# 3. Variable objetivo (cnt): NO tratar outliers ya que son valores reales de demanda\n",
    "print(f\"‚ÑπÔ∏è Variable objetivo (cnt): Outliers conservados (son valores reales de demanda alta)\")\n",
    "\n",
    "# 4. Temperatura: Sin outliers\n",
    "print(f\"‚ÑπÔ∏è Temperatura: Sin outliers detectados\")\n",
    "\n",
    "print(f\"\\nüìä Forma del dataset despu√©s del tratamiento: {df.shape}\")\n",
    "\n",
    "# Comparar estad√≠sticas antes y despu√©s\n",
    "print(\"\\nüìà COMPARACI√ìN ANTES/DESPU√âS DEL TRATAMIENTO:\")\n",
    "for col in ['hum', 'windspeed']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  Antes  - Min: {df_original[col].min():.3f}, Max: {df_original[col].max():.3f}\")\n",
    "        print(f\"  Despu√©s - Min: {df[col].min():.3f}, Max: {df[col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecac0e7",
   "metadata": {},
   "source": [
    "## 2.4 Ingenier√≠a de Caracter√≠sticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c7aa9",
   "metadata": {},
   "source": [
    "### 2.4.1 Caracter√≠sticas Temporales C√≠clicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Ingenier√≠a de Caracter√≠sticas\n",
    "print(\"üîß INGENIER√çA DE CARACTER√çSTICAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear una copia para trabajar con caracter√≠sticas\n",
    "df_features = df.copy()\n",
    "\n",
    "print(\"üìù CREANDO NUEVAS CARACTER√çSTICAS:\")\n",
    "\n",
    "# 1. Caracter√≠sticas temporales c√≠clicas\n",
    "print(\"\\nüïê Caracter√≠sticas temporales c√≠clicas:\")\n",
    "\n",
    "# Hora del d√≠a como caracter√≠stica c√≠clica\n",
    "df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hr'] / 24)\n",
    "df_features['hour_cos'] = np.cos(2 * np.pi * df_features['hr'] / 24)\n",
    "print(\"‚úÖ hour_sin, hour_cos - Representaci√≥n c√≠clica de la hora\")\n",
    "\n",
    "# Mes como caracter√≠stica c√≠clica\n",
    "df_features['month_sin'] = np.sin(2 * np.pi * df_features['mnth'] / 12)\n",
    "df_features['month_cos'] = np.cos(2 * np.pi * df_features['mnth'] / 12)\n",
    "print(\"‚úÖ month_sin, month_cos - Representaci√≥n c√≠clica del mes\")\n",
    "\n",
    "# D√≠a de la semana como caracter√≠stica c√≠clica\n",
    "df_features['weekday_sin'] = np.sin(2 * np.pi * df_features['weekday'] / 7)\n",
    "df_features['weekday_cos'] = np.cos(2 * np.pi * df_features['weekday'] / 7)\n",
    "print(\"‚úÖ weekday_sin, weekday_cos - Representaci√≥n c√≠clica del d√≠a de semana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5730b",
   "metadata": {},
   "source": [
    "### 2.4.2 Caracter√≠sticas de Interacci√≥n Clim√°tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Caracter√≠sticas de interacci√≥n clim√°tica\n",
    "print(\"\\nüå°Ô∏è Caracter√≠sticas de interacci√≥n clim√°tica:\")\n",
    "\n",
    "# √çndice de comodidad t√©rmica (temperatura ajustada por humedad)\n",
    "df_features['thermal_comfort'] = df_features['temp'] * (1 - df_features['hum'] * 0.5)\n",
    "print(\"‚úÖ thermal_comfort - √çndice de comodidad t√©rmica\")\n",
    "\n",
    "# Interacci√≥n temperatura-velocidad del viento (sensaci√≥n t√©rmica)\n",
    "df_features['wind_chill'] = df_features['temp'] * (1 - df_features['windspeed'] * 0.3)\n",
    "print(\"‚úÖ wind_chill - Sensaci√≥n t√©rmica ajustada por viento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b3934",
   "metadata": {},
   "source": [
    "### 2.4.3 Caracter√≠sticas categ√≥ricas binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd988161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Caracter√≠sticas categ√≥ricas binarias\n",
    "print(\"\\nüè∑Ô∏è Caracter√≠sticas categ√≥ricas mejoradas:\")\n",
    "\n",
    "# Horas pico (rush hours)\n",
    "df_features['is_rush_hour'] = ((df_features['hr'] >= 7) & (df_features['hr'] <= 9) | \n",
    "                              (df_features['hr'] >= 17) & (df_features['hr'] <= 19)).astype(int)\n",
    "print(\"‚úÖ is_rush_hour - Indicador de horas pico\")\n",
    "\n",
    "# Fin de semana\n",
    "df_features['is_weekend'] = (df_features['weekday'].isin([0, 6])).astype(int)\n",
    "print(\"‚úÖ is_weekend - Indicador de fin de semana\")\n",
    "\n",
    "# Estaci√≥n c√°lida (primavera-verano)\n",
    "df_features['is_warm_season'] = (df_features['season'].isin([1, 2])).astype(int)\n",
    "print(\"‚úÖ is_warm_season - Indicador de estaci√≥n c√°lida\")\n",
    "\n",
    "# Buenas condiciones clim√°ticas\n",
    "df_features['good_weather'] = (df_features['weathersit'] == 1).astype(int)\n",
    "print(\"‚úÖ good_weather - Indicador de buen clima\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f9f75",
   "metadata": {},
   "source": [
    "### 2.4.4 Caracter√≠sticas de temperatura binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf526cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Caracter√≠sticas de temperatura binned\n",
    "print(\"\\nüå°Ô∏è Caracter√≠sticas de temperatura categorizadas:\")\n",
    "\n",
    "# Categorizar temperatura en rangos\n",
    "df_features['temp_category'] = pd.cut(df_features['temp'], \n",
    "                                    bins=4, \n",
    "                                    labels=['Cold', 'Cool', 'Warm', 'Hot'])\n",
    "\n",
    "# Crear variables dummy para temperatura\n",
    "temp_dummies = pd.get_dummies(df_features['temp_category'], prefix='temp')\n",
    "df_features = pd.concat([df_features, temp_dummies], axis=1)\n",
    "df_features = df_features.drop('temp_category', axis=1)\n",
    "print(f\"‚úÖ temp_Cold, temp_Cool, temp_Warm, temp_Hot - Categor√≠as de temperatura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b5f10",
   "metadata": {},
   "source": [
    "### 2.4.5 Comparacion de Nuevos Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä Dataset original: {df.shape}\")\n",
    "print(f\"üìä Dataset con nuevas caracter√≠sticas: {df_features.shape}\")\n",
    "print(f\"üÜï Nuevas caracter√≠sticas a√±adidas: {df_features.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nüîó Nuevas variables creadas:\")\n",
    "new_features = [col for col in df_features.columns if col not in df.columns]\n",
    "for feature in new_features:\n",
    "    print(f\"  ‚Ä¢ {feature}\")\n",
    "\n",
    "# Actualizar el dataset principal\n",
    "df = df_features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3fd83",
   "metadata": {},
   "source": [
    "## 2.5 Selecci√≥n de Caracter√≠sticas\n",
    "\n",
    "**Problema identificado**: 27 caracter√≠sticas pueden ser excesivas para el dataset y causar overfitting. Vamos a aplicar t√©cnicas de selecci√≥n de caracter√≠sticas para identificar las m√°s importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde67f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5.1 An√°lisis de la situaci√≥n actual\n",
    "import numpy as np\n",
    "\n",
    "n_records = df.shape[0]\n",
    "n_features = df.shape[1] - 1  # excluye la variable objetivo\n",
    "\n",
    "# c√°lculo del m√°ximo recomendado: redondeo de la ra√≠z al decenar m√°s cercano hacia abajo\n",
    "max_features = (int(np.sqrt(n_records)) // 10) * 10\n",
    "reduction = n_features - max_features\n",
    "\n",
    "# Formateo con miles y pluralizaci√≥n\n",
    "records_fmt   = f\"{n_records:,}\"\n",
    "reduction_abs = abs(reduction)\n",
    "\n",
    "if reduction > 0:\n",
    "    status_emoji = \"‚ö†Ô∏è\"\n",
    "    status_text  = f\"Has excedido el m√°ximo recomendado en {reduction_abs} variable{'s' if reduction_abs>1 else ''}\"\n",
    "else:\n",
    "    status_emoji = \"‚úÖ\"\n",
    "    status_text  = f\"Est√°s {reduction_abs} variable{'s' if reduction_abs>1 else ''} por debajo del m√°ximo recomendado\"\n",
    "\n",
    "print(\n",
    "    f\"{status_emoji} Para {records_fmt} registros, el m√°ximo recomendado es {max_features} variables.\\n\"\n",
    "    f\"   ‚Ä¢ Actualmente tienes {n_features} variables ‚Üí {status_text}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4e9ff",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **AN√ÅLISIS DE DIMENSIONALIDAD**  \n",
    "‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì\n",
    "\n",
    "- üìä **Caracter√≠sticas actuales:** 27  \n",
    "- üìà **Registros disponibles:** 17,379  \n",
    "- üìâ **Ratio caracter√≠sticas/registros:** 0.0016  \n",
    "\n",
    "üí° **RECOMENDACI√ìN:**  \n",
    "- Para un dataset de 17,379 registros, se recomiendan hasta ~130 caracter√≠sticas  \n",
    "- Actualmente tienes 27 caracter√≠sticas  \n",
    "- No es necesaria reducci√≥n (usas 103 caracter√≠sticas menos de las recomendadas)  \n",
    "\n",
    "üîó **CARACTER√çSTICAS DISPONIBLES:**  \n",
    "1. season  \n",
    "2. yr  \n",
    "3. mnth  \n",
    "4. hr  \n",
    "5. holiday  \n",
    "6. weekday  \n",
    "7. workingday  \n",
    "8. weathersit  \n",
    "9. temp  \n",
    "10. hum  \n",
    "11. windspeed  \n",
    "12. hour_sin  \n",
    "13. hour_cos  \n",
    "14. month_sin  \n",
    "15. month_cos  \n",
    "16. weekday_sin  \n",
    "17. weekday_cos  \n",
    "18. thermal_comfort  \n",
    "19. wind_chill  \n",
    "20. is_rush_hour  \n",
    "21. is_weekend  \n",
    "22. is_warm_season  \n",
    "23. good_weather  \n",
    "24. temp_Cold  \n",
    "25. temp_Cool  \n",
    "26. temp_Warm  \n",
    "27. temp_Hot  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25243e8d",
   "metadata": {},
   "source": [
    "### 2.5.2 T√©cnicas de Selecci√≥n de Caracter√≠sticas\n",
    "\n",
    "Aunque el n√∫mero actual de caracter√≠sticas est√° dentro del rango recomendado, aplicaremos t√©cnicas de selecci√≥n para:\n",
    "- **Mejorar el rendimiento del modelo** (reducir tiempo de entrenamiento)\n",
    "- **Eliminar caracter√≠sticas redundantes** o poco importantes\n",
    "- **Reducir el riesgo de overfitting**\n",
    "- **Facilitar la interpretabilidad** del modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5.2 Implementaci√≥n de T√©cnicas de Selecci√≥n de Caracter√≠sticas\n",
    "print(\"üîç SELECCI√ìN DE CARACTER√çSTICAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Separar caracter√≠sticas y variable objetivo\n",
    "X = df.drop('cnt', axis=1)\n",
    "y = df['cnt']\n",
    "\n",
    "print(f\"üìä Caracter√≠sticas actuales: {X.shape[1]}\")\n",
    "print(f\"üéØ Variable objetivo: {y.name}\")\n",
    "\n",
    "# 1. SELECCI√ìN BASADA EN CORRELACI√ìN\n",
    "print(\"\\nüîó 1. SELECCI√ìN BASADA EN CORRELACI√ìN CON LA VARIABLE OBJETIVO\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calcular correlaciones con la variable objetivo\n",
    "correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "print(\"üìà Top 15 caracter√≠sticas m√°s correlacionadas:\")\n",
    "for i, (feature, corr) in enumerate(correlations.head(15).items(), 1):\n",
    "    print(f\"{i:2d}. {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "# Seleccionar las caracter√≠sticas m√°s correlacionadas (umbral >= 0.1)\n",
    "high_corr_features = correlations[correlations >= 0.1].index.tolist()\n",
    "print(f\"\\n‚úÖ Caracter√≠sticas con correlaci√≥n >= 0.1: {len(high_corr_features)}\")\n",
    "\n",
    "# 2. DETECCI√ìN DE MULTICOLINEALIDAD ENTRE CARACTER√çSTICAS\n",
    "print(\"\\nüîÑ 2. DETECCI√ìN DE MULTICOLINEALIDAD ENTRE CARACTER√çSTICAS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calcular matriz de correlaci√≥n entre caracter√≠sticas\n",
    "feature_corr_matrix = X.corr()\n",
    "highly_corr_pairs = []\n",
    "\n",
    "for i in range(len(feature_corr_matrix.columns)):\n",
    "    for j in range(i+1, len(feature_corr_matrix.columns)):\n",
    "        corr_val = abs(feature_corr_matrix.iloc[i, j])\n",
    "        if corr_val > 0.8:  # Umbral para alta correlaci√≥n\n",
    "            highly_corr_pairs.append((\n",
    "                feature_corr_matrix.columns[i], \n",
    "                feature_corr_matrix.columns[j], \n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "print(f\"‚ö†Ô∏è Pares de caracter√≠sticas altamente correlacionadas (>0.8): {len(highly_corr_pairs)}\")\n",
    "for feat1, feat2, corr_val in highly_corr_pairs:\n",
    "    print(f\"  ‚Ä¢ {feat1} - {feat2}: {corr_val:.3f}\")\n",
    "\n",
    "# 3. SELECCI√ìN DE CARACTER√çSTICAS FINALES\n",
    "print(\"\\nüìã 3. ESTRATEGIA DE SELECCI√ìN FINAL\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Comenzar con las caracter√≠sticas m√°s correlacionadas\n",
    "selected_features = high_corr_features.copy()\n",
    "\n",
    "# Remover caracter√≠sticas redundantes por multicolinealidad\n",
    "features_to_remove = []\n",
    "for feat1, feat2, corr_val in highly_corr_pairs:\n",
    "    if feat1 in selected_features and feat2 in selected_features:\n",
    "        # Mantener la que tenga mayor correlaci√≥n con la variable objetivo\n",
    "        if correlations[feat1] >= correlations[feat2]:\n",
    "            features_to_remove.append(feat2)\n",
    "        else:\n",
    "            features_to_remove.append(feat1)\n",
    "\n",
    "# Remover caracter√≠sticas duplicadas identificadas\n",
    "selected_features = [f for f in selected_features if f not in features_to_remove]\n",
    "\n",
    "print(f\"üóëÔ∏è Caracter√≠sticas removidas por multicolinealidad: {len(features_to_remove)}\")\n",
    "for feat in features_to_remove:\n",
    "    print(f\"  ‚Ä¢ {feat}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Caracter√≠sticas seleccionadas: {len(selected_features)}\")\n",
    "print(f\"üìâ Reducci√≥n: {X.shape[1]} ‚Üí {len(selected_features)} caracter√≠sticas ({X.shape[1] - len(selected_features)} removidas)\")\n",
    "\n",
    "# Crear dataset con caracter√≠sticas seleccionadas\n",
    "X_selected = X[selected_features]\n",
    "df_selected = pd.concat([X_selected, y], axis=1)\n",
    "\n",
    "print(f\"\\nüìä Forma del dataset optimizado: {df_selected.shape}\")\n",
    "print(f\"üéØ Ratio caracter√≠sticas/registros: {len(selected_features)/len(df_selected):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44800178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. VISUALIZACI√ìN DE CARACTER√çSTICAS SELECCIONADAS\n",
    "print(\"\\nüìä 4. VISUALIZACI√ìN DE CARACTER√çSTICAS SELECCIONADAS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Crear visualizaci√≥n de importancia\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlations_selected = correlations[selected_features].sort_values(ascending=True)\n",
    "\n",
    "plt.barh(range(len(correlations_selected)), correlations_selected.values, \n",
    "         color='skyblue', alpha=0.8)\n",
    "plt.yticks(range(len(correlations_selected)), correlations_selected.index)\n",
    "plt.xlabel('Correlaci√≥n Absoluta con Variable Objetivo')\n",
    "plt.title('Caracter√≠sticas Seleccionadas - Importancia por Correlaci√≥n')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for i, v in enumerate(correlations_selected.values):\n",
    "    plt.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. AN√ÅLISIS COMPARATIVO\n",
    "print(\"\\nüìà 5. AN√ÅLISIS COMPARATIVO: ANTES vs DESPU√âS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "comparison_data = {\n",
    "    'M√©trica': [\n",
    "        'N√∫mero de caracter√≠sticas',\n",
    "        'Correlaci√≥n promedio con target',\n",
    "        'Correlaci√≥n m√°xima con target',\n",
    "        'Correlaci√≥n m√≠nima con target',\n",
    "        'Caracter√≠sticas con corr > 0.2',\n",
    "        'Caracter√≠sticas con corr > 0.3'\n",
    "    ],\n",
    "    'Antes': [\n",
    "        X.shape[1],\n",
    "        X.corrwith(y).abs().mean(),\n",
    "        X.corrwith(y).abs().max(),\n",
    "        X.corrwith(y).abs().min(),\n",
    "        (X.corrwith(y).abs() > 0.2).sum(),\n",
    "        (X.corrwith(y).abs() > 0.3).sum()\n",
    "    ],\n",
    "    'Despu√©s': [\n",
    "        len(selected_features),\n",
    "        correlations[selected_features].mean(),\n",
    "        correlations[selected_features].max(),\n",
    "        correlations[selected_features].min(),\n",
    "        (correlations[selected_features] > 0.2).sum(),\n",
    "        (correlations[selected_features] > 0.3).sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 6. ACTUALIZAR DATASET PRINCIPAL\n",
    "print(\"\\nüîÑ 6. ACTUALIZACI√ìN DEL DATASET PRINCIPAL\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Actualizar el dataset principal con las caracter√≠sticas seleccionadas\n",
    "df = df_selected.copy()\n",
    "\n",
    "print(f\"‚úÖ Dataset actualizado exitosamente\")\n",
    "print(f\"üìä Forma final: {df.shape}\")\n",
    "print(f\"üéØ Caracter√≠sticas finales: {len(selected_features)}\")\n",
    "print(f\"‚ö° Reducci√≥n de dimensionalidad: {((X.shape[1] - len(selected_features)) / X.shape[1] * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nüîó CARACTER√çSTICAS FINALES SELECCIONADAS:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    corr_val = correlations[feature]\n",
    "    print(f\"{i:2d}. {feature:<20}: {corr_val:.3f}\")\n",
    "\n",
    "print(f\"\\nüöÄ Dataset optimizado y listo para modelado!\")\n",
    "print(f\"   ‚Ä¢ Caracter√≠sticas reducidas para mejor rendimiento\")\n",
    "print(f\"   ‚Ä¢ Multicolinealidad eliminada\") \n",
    "print(f\"   ‚Ä¢ Solo caracter√≠sticas relevantes mantenidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5735e98",
   "metadata": {},
   "source": [
    "## 2.6 Resumen de la Fase de Preprocesamiento\n",
    "\n",
    "### ‚úÖ **TRANSFORMACIONES COMPLETADAS:**\n",
    "\n",
    "#### **üîß Tratamiento de Datos:**\n",
    "- ‚úÖ **Multicolinealidad resuelta**: Eliminaci√≥n de `atemp` (r=0.988 con `temp`)\n",
    "- ‚úÖ **Outliers tratados**: Capping aplicado a `windspeed` y `hum`\n",
    "- ‚úÖ **Calidad de datos**: Dataset limpio y consistente\n",
    "\n",
    "#### **‚ö° Ingenier√≠a de Caracter√≠sticas:**\n",
    "- ‚úÖ **Caracter√≠sticas temporales c√≠clicas**: `hour_sin/cos`, `month_sin/cos`, `weekday_sin/cos`\n",
    "- ‚úÖ **Interacciones clim√°ticas**: `thermal_comfort`, `wind_chill`\n",
    "- ‚úÖ **Caracter√≠sticas binarias**: `is_rush_hour`, `is_weekend`, `is_warm_season`, `good_weather`\n",
    "- ‚úÖ **Categorizaci√≥n**: Variables dummy para temperatura\n",
    "\n",
    "#### **üéØ Selecci√≥n de Caracter√≠sticas:**\n",
    "- ‚úÖ **Optimizaci√≥n dimensional**: Reducci√≥n basada en correlaci√≥n y relevancia\n",
    "- ‚úÖ **Eliminaci√≥n de redundancia**: Caracter√≠sticas multicolineales removidas\n",
    "- ‚úÖ **Mejora de rendimiento**: Dataset optimizado para modelado eficiente\n",
    "\n",
    "### **üìä ESTADO FINAL DEL DATASET:**\n",
    "- **Registros**: 17,379 (sin p√©rdida de datos)\n",
    "- **Caracter√≠sticas**: Optimizadas para rendimiento\n",
    "- **Calidad**: Excelente, listo para modelado\n",
    "- **Rendimiento**: Mejorado para entrenamiento r√°pido\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ PR√ìXIMOS PASOS - FASE 3: MODELADO**\n",
    "1. **Divisi√≥n del dataset** (Train/Test split)\n",
    "2. **Escalado de caracter√≠sticas** (si es necesario)\n",
    "3. **Entrenamiento de modelos** m√∫ltiples\n",
    "4. **Evaluaci√≥n y comparaci√≥n** de resultados\n",
    "5. **Selecci√≥n del mejor modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb1c99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# FASE 3: MODELADO Y EVALUACI√ìN\n",
    "\n",
    "## 3.1 Preparaci√≥n para Modelado\n",
    "\n",
    "### üéØ **ESTRATEGIA DE MODELADO**\n",
    "Con el dataset optimizado, procederemos a entrenar m√∫ltiples modelos de regresi√≥n para comparar su rendimiento:\n",
    "\n",
    "**Modelos a evaluar:**\n",
    "1. **Support Vector Machine (SVM)**\n",
    "2. **Decision Tree**\n",
    "3. **Random Forest**\n",
    "4. **XGBoost**\n",
    "5. **Redes Neuronales (MLP)**\n",
    "6. **Bagging**\n",
    "7. **Gradient Boosting**\n",
    "8. **Voting Regressor**\n",
    "\n",
    "### ‚ö° **VENTAJAS DEL DATASET ACTUAL**\n",
    "- **Dimensionalidad optimizada**: Entrenamiento m√°s r√°pido\n",
    "- **Caracter√≠sticas relevantes**: Mayor precisi√≥n\n",
    "- **Sin multicolinealidad**: Estabilidad en SVM y redes neuronales\n",
    "- **Calidad de datos**: Excelente para todos los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559459c",
   "metadata": {},
   "source": [
    "## 3.2 Divisi√≥n del Dataset\n",
    "\n",
    "### üîÑ **Preparaci√≥n para Entrenamiento**\n",
    "Dividiremos el dataset en conjuntos de entrenamiento y prueba, y aplicaremos escalado si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Divisi√≥n del Dataset y Preparaci√≥n para Modelado\n",
    "print(\"üîÑ DIVISI√ìN DEL DATASET Y PREPARACI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Separar caracter√≠sticas y variable objetivo\n",
    "X = df.drop('cnt', axis=1)\n",
    "y = df['cnt']\n",
    "\n",
    "print(f\"üìä Forma del dataset completo: {df.shape}\")\n",
    "print(f\"üéØ Variables predictoras: {X.shape[1]}\")\n",
    "print(f\"üìà Variable objetivo: {y.name}\")\n",
    "\n",
    "# Divisi√≥n estratificada del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.25, \n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä DIVISI√ìN DEL DATASET:\")\n",
    "print(f\"  ‚Ä¢ Entrenamiento: {X_train.shape[0]} registros ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Prueba: {X_test.shape[0]} registros ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribuciones\n",
    "print(f\"\\nüìà DISTRIBUCI√ìN DE LA VARIABLE OBJETIVO:\")\n",
    "print(f\"  ‚Ä¢ Entrenamiento - Media: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"  ‚Ä¢ Prueba - Media: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "\n",
    "# Escalado de caracter√≠sticas (necesario para SVM y Redes Neuronales)\n",
    "print(f\"\\nüîß ESCALADO DE CARACTER√çSTICAS:\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertir de vuelta a DataFrame para mantener nombres de columnas\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"‚úÖ Escalado aplicado usando StandardScaler\")\n",
    "print(f\"  ‚Ä¢ Media de caracter√≠sticas escaladas: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"  ‚Ä¢ Desviaci√≥n est√°ndar de caracter√≠sticas escaladas: {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "# Mostrar caracter√≠sticas finales\n",
    "print(f\"\\nüîó CARACTER√çSTICAS FINALES PARA MODELADO:\")\n",
    "for i, feature in enumerate(X.columns, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nüöÄ Datos preparados para entrenamiento de modelos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33fb142",
   "metadata": {},
   "source": [
    "## 3.3 Entrenamiento de Modelos M√∫ltiples\n",
    "\n",
    "### ü§ñ **Suite de Modelos de Regresi√≥n**\n",
    "Entrenaremos 8 modelos diferentes para comparar su rendimiento:\n",
    "\n",
    "1. **üîß Support Vector Machine (SVM)** - Modelo robusto con kernel RBF\n",
    "2. **üå≥ Decision Tree** - Modelo interpretable basado en √°rboles\n",
    "3. **üå≤ Random Forest** - Ensemble de √°rboles de decisi√≥n\n",
    "4. **‚ö° XGBoost** - Gradient Boosting optimizado\n",
    "5. **üß† Redes Neuronales (MLP)** - Multi-Layer Perceptron\n",
    "6. **üì¶ Bagging** - Bootstrap Aggregating\n",
    "7. **üöÄ Gradient Boosting** - Boosting secuencial\n",
    "8. **üó≥Ô∏è Voting Regressor** - Combinaci√≥n de m√∫ltiples modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Entrenamiento de Modelos M√∫ltiples\n",
    "print(\"ü§ñ ENTRENAMIENTO DE MODELOS M√öLTIPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "# Diccionario para almacenar los modelos entrenados\n",
    "models = {}\n",
    "training_times = {}\n",
    "model_results = {}\n",
    "\n",
    "# Configuraciones base para los modelos\n",
    "print(\"üîß Configurando modelos...\")\n",
    "\n",
    "# 1. Support Vector Machine (SVM)\n",
    "print(\"\\n1Ô∏è‚É£ Entrenando Support Vector Machine (SVM)...\")\n",
    "start_time = time.time()\n",
    "svm_model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "training_times['SVM'] = time.time() - start_time\n",
    "models['SVM'] = svm_model\n",
    "model_results['SVM'] = evaluate_regression_model(svm_model, X_test_scaled, y_test, y_pred_svm, 'SVM')\n",
    "print(f\"‚úÖ SVM entrenado en {training_times['SVM']:.2f} segundos\")\n",
    "\n",
    "# 2. Decision Tree\n",
    "print(\"\\n2Ô∏è‚É£ Entrenando Decision Tree...\")\n",
    "start_time = time.time()\n",
    "dt_model = DecisionTreeRegressor(max_depth=15, min_samples_split=10, min_samples_leaf=5, random_state=SEED)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "training_times['Decision Tree'] = time.time() - start_time\n",
    "models['Decision Tree'] = dt_model\n",
    "model_results['Decision Tree'] = evaluate_regression_model(dt_model, X_test, y_test, y_pred_dt, 'Decision Tree')\n",
    "print(f\"‚úÖ Decision Tree entrenado en {training_times['Decision Tree']:.2f} segundos\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3Ô∏è‚É£ Entrenando Random Forest...\")\n",
    "start_time = time.time()\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=10, \n",
    "                               min_samples_leaf=5, random_state=SEED, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "training_times['Random Forest'] = time.time() - start_time\n",
    "models['Random Forest'] = rf_model\n",
    "model_results['Random Forest'] = evaluate_regression_model(rf_model, X_test, y_test, y_pred_rf, 'Random Forest')\n",
    "print(f\"‚úÖ Random Forest entrenado en {training_times['Random Forest']:.2f} segundos\")\n",
    "\n",
    "# 4. XGBoost\n",
    "print(\"\\n4Ô∏è‚É£ Entrenando XGBoost...\")\n",
    "start_time = time.time()\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                           subsample=0.8, colsample_bytree=0.8, random_state=SEED, n_jobs=-1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "training_times['XGBoost'] = time.time() - start_time\n",
    "models['XGBoost'] = xgb_model\n",
    "model_results['XGBoost'] = evaluate_regression_model(xgb_model, X_test, y_test, y_pred_xgb, 'XGBoost')\n",
    "print(f\"‚úÖ XGBoost entrenado en {training_times['XGBoost']:.2f} segundos\")\n",
    "\n",
    "# 5. Redes Neuronales (MLP)\n",
    "print(\"\\n5Ô∏è‚É£ Entrenando Redes Neuronales (MLP)...\")\n",
    "start_time = time.time()\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, alpha=0.01, \n",
    "                        random_state=SEED, early_stopping=True, validation_fraction=0.1)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "y_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "training_times['Neural Network'] = time.time() - start_time\n",
    "models['Neural Network'] = mlp_model\n",
    "model_results['Neural Network'] = evaluate_regression_model(mlp_model, X_test_scaled, y_test, y_pred_mlp, 'Neural Network')\n",
    "print(f\"‚úÖ Neural Network entrenado en {training_times['Neural Network']:.2f} segundos\")\n",
    "\n",
    "# 6. Bagging\n",
    "print(\"\\n6Ô∏è‚É£ Entrenando Bagging...\")\n",
    "start_time = time.time()\n",
    "bagging_model = BaggingRegressor(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "training_times['Bagging'] = time.time() - start_time\n",
    "models['Bagging'] = bagging_model\n",
    "model_results['Bagging'] = evaluate_regression_model(bagging_model, X_test, y_test, y_pred_bagging, 'Bagging')\n",
    "print(f\"‚úÖ Bagging entrenado en {training_times['Bagging']:.2f} segundos\")\n",
    "\n",
    "# 7. Gradient Boosting\n",
    "print(\"\\n7Ô∏è‚É£ Entrenando Gradient Boosting...\")\n",
    "start_time = time.time()\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                                   min_samples_split=10, min_samples_leaf=5, random_state=SEED)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "training_times['Gradient Boosting'] = time.time() - start_time\n",
    "models['Gradient Boosting'] = gb_model\n",
    "model_results['Gradient Boosting'] = evaluate_regression_model(gb_model, X_test, y_test, y_pred_gb, 'Gradient Boosting')\n",
    "print(f\"‚úÖ Gradient Boosting entrenado en {training_times['Gradient Boosting']:.2f} segundos\")\n",
    "\n",
    "# 8. Voting Regressor\n",
    "print(\"\\n8Ô∏è‚É£ Entrenando Voting Regressor...\")\n",
    "start_time = time.time()\n",
    "# Usar los mejores modelos para el ensemble\n",
    "voting_model = VotingRegressor([\n",
    "    ('rf', RandomForestRegressor(n_estimators=50, max_depth=15, random_state=SEED)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=50, max_depth=6, random_state=SEED)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, random_state=SEED))\n",
    "], n_jobs=-1)\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "training_times['Voting'] = time.time() - start_time\n",
    "models['Voting'] = voting_model\n",
    "model_results['Voting'] = evaluate_regression_model(voting_model, X_test, y_test, y_pred_voting, 'Voting')\n",
    "print(f\"‚úÖ Voting entrenado en {training_times['Voting']:.2f} segundos\")\n",
    "\n",
    "print(f\"\\nüéâ ¬°Todos los modelos entrenados exitosamente!\")\n",
    "print(f\"‚è±Ô∏è Tiempo total de entrenamiento: {sum(training_times.values()):.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e3105",
   "metadata": {},
   "source": [
    "## 3.4 Evaluaci√≥n y Comparaci√≥n de Modelos\n",
    "\n",
    "### üìä **An√°lisis de Resultados**\n",
    "Evaluaremos el rendimiento de todos los modelos usando m√∫ltiples m√©tricas de regresi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Evaluaci√≥n y Comparaci√≥n de Modelos\n",
    "print(\"üìä EVALUACI√ìN Y COMPARACI√ìN DE MODELOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Tabla de m√©tricas de rendimiento\n",
    "print(\"üìà TABLA DE M√âTRICAS DE RENDIMIENTO:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Crear tabla resumen de m√©tricas\n",
    "metrics_summary = []\n",
    "for model_name, results in model_results.items():\n",
    "    metrics_summary.append({\n",
    "        'Modelo': model_name,\n",
    "        'R¬≤': results['R¬≤'],\n",
    "        'RMSE': results['RMSE'],\n",
    "        'MAE': results['MAE'],\n",
    "        'MAPE': results['MAPE'],\n",
    "        'Tiempo (s)': training_times[model_name]\n",
    "    })\n",
    "\n",
    "# Crear DataFrame y ordenar por R¬≤\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "metrics_df = metrics_df.sort_values('R¬≤', ascending=False)\n",
    "\n",
    "# Mostrar tabla formateada\n",
    "print(metrics_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# 2. Identificar el mejor modelo\n",
    "best_model_name = metrics_df.iloc[0]['Modelo']\n",
    "best_r2 = metrics_df.iloc[0]['R¬≤']\n",
    "best_rmse = metrics_df.iloc[0]['RMSE']\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ R¬≤: {best_r2:.3f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {best_rmse:.3f}\")\n",
    "print(f\"   ‚Ä¢ Tiempo de entrenamiento: {training_times[best_model_name]:.2f}s\")\n",
    "\n",
    "# 3. An√°lisis de rendimiento\n",
    "print(f\"\\nüìä AN√ÅLISIS DE RENDIMIENTO:\")\n",
    "print(f\"   ‚Ä¢ Mejor R¬≤: {metrics_df['R¬≤'].max():.3f} ({metrics_df.loc[metrics_df['R¬≤'].idxmax(), 'Modelo']})\")\n",
    "print(f\"   ‚Ä¢ Menor RMSE: {metrics_df['RMSE'].min():.3f} ({metrics_df.loc[metrics_df['RMSE'].idxmin(), 'Modelo']})\")\n",
    "print(f\"   ‚Ä¢ Menor MAE: {metrics_df['MAE'].min():.3f} ({metrics_df.loc[metrics_df['MAE'].idxmin(), 'Modelo']})\")\n",
    "print(f\"   ‚Ä¢ Mayor velocidad: {metrics_df['Tiempo (s)'].min():.2f}s ({metrics_df.loc[metrics_df['Tiempo (s)'].idxmin(), 'Modelo']})\")\n",
    "\n",
    "# 4. Categorizaci√≥n de modelos\n",
    "print(f\"\\nüéØ CATEGORIZACI√ìN DE MODELOS:\")\n",
    "high_performance = metrics_df[metrics_df['R¬≤'] > 0.8]['Modelo'].tolist()\n",
    "medium_performance = metrics_df[(metrics_df['R¬≤'] > 0.7) & (metrics_df['R¬≤'] <= 0.8)]['Modelo'].tolist()\n",
    "low_performance = metrics_df[metrics_df['R¬≤'] <= 0.7]['Modelo'].tolist()\n",
    "\n",
    "print(f\"   ü•á Alto rendimiento (R¬≤ > 0.8): {high_performance}\")\n",
    "print(f\"   ü•à Rendimiento medio (0.7 < R¬≤ ‚â§ 0.8): {medium_performance}\")\n",
    "print(f\"   ü•â Rendimiento bajo (R¬≤ ‚â§ 0.7): {low_performance}\")\n",
    "\n",
    "# 5. Recomendaciones\n",
    "print(f\"\\nüí° RECOMENDACIONES:\")\n",
    "if best_r2 > 0.85:\n",
    "    print(f\"   ‚úÖ Excelente rendimiento del modelo {best_model_name}\")\n",
    "elif best_r2 > 0.75:\n",
    "    print(f\"   ‚úÖ Buen rendimiento del modelo {best_model_name}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Rendimiento mejorable del modelo {best_model_name}\")\n",
    "\n",
    "# An√°lisis de eficiencia\n",
    "fastest_model = metrics_df.loc[metrics_df['Tiempo (s)'].idxmin(), 'Modelo']\n",
    "fastest_time = metrics_df['Tiempo (s)'].min()\n",
    "print(f\"   ‚ö° Modelo m√°s r√°pido: {fastest_model} ({fastest_time:.2f}s)\")\n",
    "\n",
    "# An√°lisis de trade-off rendimiento vs velocidad\n",
    "print(f\"\\n‚öñÔ∏è AN√ÅLISIS RENDIMIENTO vs VELOCIDAD:\")\n",
    "for _, row in metrics_df.head(3).iterrows():\n",
    "    efficiency = row['R¬≤'] / row['Tiempo (s)']\n",
    "    print(f\"   ‚Ä¢ {row['Modelo']}: R¬≤={row['R¬≤']:.3f}, Tiempo={row['Tiempo (s)']:.2f}s, Eficiencia={efficiency:.3f}\")\n",
    "\n",
    "print(f\"\\nüîç Preparando visualizaciones detalladas...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c838a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Visualizaciones Comparativas\n",
    "print(\"üìä GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Gr√°fico de m√©tricas comparativas\n",
    "print(\"üìà Generando gr√°fico de m√©tricas comparativas...\")\n",
    "metrics_comparison_df = compare_models_metrics(model_results, figsize=(15, 10))\n",
    "\n",
    "# 2. Gr√°fico de predicciones vs valores reales\n",
    "print(\"üéØ Generando gr√°fico de predicciones vs valores reales...\")\n",
    "plot_regression_results(model_results, y_test, figsize=(20, 15))\n",
    "\n",
    "# 3. An√°lisis de residuos\n",
    "print(\"üìâ Generando an√°lisis de residuos...\")\n",
    "plot_residuals_analysis(model_results, y_test, figsize=(20, 15))\n",
    "\n",
    "# 4. Gr√°fico de tiempos de entrenamiento\n",
    "print(\"‚è±Ô∏è Generando gr√°fico de tiempos de entrenamiento...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "models_names = list(training_times.keys())\n",
    "times = list(training_times.values())\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "bars = plt.bar(models_names, times, color='lightcoral', alpha=0.8)\n",
    "plt.title('Tiempos de Entrenamiento por Modelo')\n",
    "plt.ylabel('Tiempo (segundos)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for bar, time in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{time:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 5. Gr√°fico de eficiencia (R¬≤ vs Tiempo)\n",
    "plt.subplot(1, 2, 2)\n",
    "r2_scores = [model_results[model]['R¬≤'] for model in models_names]\n",
    "plt.scatter(times, r2_scores, s=100, alpha=0.7, c='skyblue', edgecolor='black')\n",
    "\n",
    "for i, model in enumerate(models_names):\n",
    "    plt.annotate(model, (times[i], r2_scores[i]), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.xlabel('Tiempo de Entrenamiento (segundos)')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Eficiencia: R¬≤ vs Tiempo de Entrenamiento')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Ranking final de modelos\n",
    "print(\"\\nüèÜ RANKING FINAL DE MODELOS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear ranking ponderado (70% rendimiento, 30% velocidad)\n",
    "ranking_data = []\n",
    "for model_name in models_names:\n",
    "    r2_score = model_results[model_name]['R¬≤']\n",
    "    time_score = 1 / (training_times[model_name] + 0.1)  # Inverso del tiempo\n",
    "    \n",
    "    # Normalizar scores\n",
    "    r2_norm = r2_score / max(r2_scores)\n",
    "    time_norm = time_score / max([1 / (t + 0.1) for t in times])\n",
    "    \n",
    "    # Score ponderado\n",
    "    weighted_score = 0.7 * r2_norm + 0.3 * time_norm\n",
    "    \n",
    "    ranking_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'R¬≤': r2_score,\n",
    "        'RMSE': model_results[model_name]['RMSE'],\n",
    "        'MAE': model_results[model_name]['MAE'],\n",
    "        'Tiempo': training_times[model_name],\n",
    "        'Score_Ponderado': weighted_score\n",
    "    })\n",
    "\n",
    "# Ordenar por score ponderado\n",
    "ranking_df = pd.DataFrame(ranking_data)\n",
    "ranking_df = ranking_df.sort_values('Score_Ponderado', ascending=False)\n",
    "\n",
    "print(\"üéñÔ∏è RANKING PONDERADO (70% Rendimiento + 30% Velocidad):\")\n",
    "for i, (_, row) in enumerate(ranking_df.iterrows(), 1):\n",
    "    print(f\"{i}. {row['Modelo']:<15} | R¬≤: {row['R¬≤']:.3f} | RMSE: {row['RMSE']:.2f} | Tiempo: {row['Tiempo']:.2f}s | Score: {row['Score_Ponderado']:.3f}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model_final = ranking_df.iloc[0]['Modelo']\n",
    "best_model_obj = models[best_model_final]\n",
    "\n",
    "print(f\"\\nü•á MODELO GANADOR: {best_model_final}\")\n",
    "print(f\"   üìä R¬≤: {ranking_df.iloc[0]['R¬≤']:.3f}\")\n",
    "print(f\"   üìâ RMSE: {ranking_df.iloc[0]['RMSE']:.2f}\")\n",
    "print(f\"   ‚è±Ô∏è Tiempo: {ranking_df.iloc[0]['Tiempo']:.2f}s\")\n",
    "print(f\"   üéØ Score ponderado: {ranking_df.iloc[0]['Score_Ponderado']:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluaci√≥n completa terminada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea135128",
   "metadata": {},
   "source": [
    "## 3.5 Cross Validation de Todos los Modelos\n",
    "\n",
    "### üîÑ **Validaci√≥n Cruzada Completa**\n",
    "Realizaremos validaci√≥n cruzada con K-Fold (k=5) para evaluar la robustez y generalizaci√≥n de todos los modelos entrenados. Esto nos permitir√°:\n",
    "\n",
    "- **Evaluar la estabilidad** de cada modelo\n",
    "- **Detectar overfitting/underfitting**\n",
    "- **Comparar el rendimiento promedio** y la variabilidad\n",
    "- **Obtener intervalos de confianza** para las m√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Cross Validation de Todos los Modelos\n",
    "print(\"üîÑ CROSS VALIDATION DE TODOS LOS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time as time_module\n",
    "\n",
    "# Configurar K-Fold\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "print(f\"üìã Configuraci√≥n de Cross Validation:\")\n",
    "print(f\"  ‚Ä¢ M√©todo: K-Fold\")\n",
    "print(f\"  ‚Ä¢ N√∫mero de folds: {k_folds}\")\n",
    "print(f\"  ‚Ä¢ Semilla aleatoria: {SEED}\")\n",
    "print(f\"  ‚Ä¢ Dataset completo: {X.shape[0]} registros\")\n",
    "\n",
    "# M√©tricas a evaluar\n",
    "scoring_metrics = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "\n",
    "# Definir modelos para cross validation (recreamos para evitar problemas)\n",
    "print(f\"\\nü§ñ Modelos a evaluar:\")\n",
    "\n",
    "cv_models = {\n",
    "    'SVM': SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=15, min_samples_split=10, min_samples_leaf=5, random_state=SEED),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=10, \n",
    "                                         min_samples_leaf=5, random_state=SEED, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                              subsample=0.8, colsample_bytree=0.8, random_state=SEED, n_jobs=-1),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, alpha=0.01, \n",
    "                                 random_state=SEED, early_stopping=True, validation_fraction=0.1),\n",
    "    'Bagging': BaggingRegressor(n_estimators=100, random_state=SEED, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                                                 min_samples_split=10, min_samples_leaf=5, random_state=SEED),\n",
    "    'Voting': VotingRegressor([\n",
    "        ('rf', RandomForestRegressor(n_estimators=50, max_depth=15, random_state=SEED)),\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=50, max_depth=6, random_state=SEED)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, random_state=SEED))\n",
    "    ], n_jobs=-1)\n",
    "}\n",
    "\n",
    "for i, model_name in enumerate(cv_models.keys(), 1):\n",
    "    print(f\"  {i}. {model_name}\")\n",
    "\n",
    "# Almacenar resultados de CV\n",
    "cv_results = {}\n",
    "cv_times = {}\n",
    "\n",
    "print(f\"\\nüöÄ Iniciando Cross Validation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Realizar cross validation para cada modelo\n",
    "for model_name, model in cv_models.items():\n",
    "    print(f\"\\nüîÑ Cross Validation - {model_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    start_time = time_module.time()\n",
    "    \n",
    "    try:\n",
    "        # Determinar si usar datos escalados o no\n",
    "        if model_name in ['SVM', 'Neural Network']:\n",
    "            # Para modelos que necesitan escalado, usar todo el dataset escalado\n",
    "            X_cv = pd.concat([X_train_scaled, X_test_scaled]).sort_index()\n",
    "            y_cv = pd.concat([y_train, y_test]).sort_index()\n",
    "            print(f\"  üìä Usando datos escalados\")\n",
    "        else:\n",
    "            # Para otros modelos, usar datos originales\n",
    "            X_cv = X\n",
    "            y_cv = y\n",
    "            print(f\"  üìä Usando datos originales\")\n",
    "        \n",
    "        # Realizar cross validation\n",
    "        cv_scores = cross_validate(\n",
    "            model, X_cv, y_cv, \n",
    "            cv=kfold, \n",
    "            scoring=scoring_metrics,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        r2_scores = cv_scores['test_r2']\n",
    "        rmse_scores = np.sqrt(-cv_scores['test_neg_mean_squared_error'])\n",
    "        mae_scores = -cv_scores['test_neg_mean_absolute_error']\n",
    "        \n",
    "        # Guardar resultados\n",
    "        cv_results[model_name] = {\n",
    "            'R¬≤_scores': r2_scores,\n",
    "            'RMSE_scores': rmse_scores,\n",
    "            'MAE_scores': mae_scores,\n",
    "            'R¬≤_mean': np.mean(r2_scores),\n",
    "            'R¬≤_std': np.std(r2_scores),\n",
    "            'RMSE_mean': np.mean(rmse_scores),\n",
    "            'RMSE_std': np.std(rmse_scores),\n",
    "            'MAE_mean': np.mean(mae_scores),\n",
    "            'MAE_std': np.std(mae_scores),\n",
    "            'train_r2_mean': np.mean(cv_scores['train_r2']),\n",
    "            'train_r2_std': np.std(cv_scores['train_r2'])\n",
    "        }\n",
    "        \n",
    "        cv_times[model_name] = time_module.time() - start_time\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(f\"  ‚úÖ Completado en {cv_times[model_name]:.2f}s\")\n",
    "        print(f\"  üìä R¬≤ = {cv_results[model_name]['R¬≤_mean']:.4f} ¬± {cv_results[model_name]['R¬≤_std']:.4f}\")\n",
    "        print(f\"  üìä RMSE = {cv_results[model_name]['RMSE_mean']:.2f} ¬± {cv_results[model_name]['RMSE_std']:.2f}\")\n",
    "        print(f\"  üìä MAE = {cv_results[model_name]['MAE_mean']:.2f} ¬± {cv_results[model_name]['MAE_std']:.2f}\")\n",
    "        \n",
    "        # Detectar overfitting\n",
    "        train_test_diff = cv_results[model_name]['train_r2_mean'] - cv_results[model_name]['R¬≤_mean']\n",
    "        if train_test_diff > 0.1:\n",
    "            print(f\"  ‚ö†Ô∏è  Posible overfitting detectado (diff: {train_test_diff:.3f})\")\n",
    "        elif train_test_diff < 0.05:\n",
    "            print(f\"  ‚úÖ Buena generalizaci√≥n (diff: {train_test_diff:.3f})\")\n",
    "        else:\n",
    "            print(f\"  üìä Generalizaci√≥n moderada (diff: {train_test_diff:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error en {model_name}: {str(e)}\")\n",
    "        cv_results[model_name] = None\n",
    "\n",
    "print(f\"\\nüéâ Cross Validation completada!\")\n",
    "print(f\"‚è±Ô∏è Tiempo total: {sum(cv_times.values()):.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla comparativa de Cross Validation\n",
    "print(\"\\nüìä TABLA COMPARATIVA - CROSS VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear DataFrame con resultados de CV\n",
    "cv_summary = []\n",
    "for model_name, results in cv_results.items():\n",
    "    if results is not None:\n",
    "        cv_summary.append({\n",
    "            'Modelo': model_name,\n",
    "            'R¬≤ Mean': results['R¬≤_mean'],\n",
    "            'R¬≤ Std': results['R¬≤_std'],\n",
    "            'RMSE Mean': results['RMSE_mean'],\n",
    "            'RMSE Std': results['RMSE_std'],\n",
    "            'MAE Mean': results['MAE_mean'],\n",
    "            'MAE Std': results['MAE_std'],\n",
    "            'Train R¬≤': results['train_r2_mean'],\n",
    "            'Overfitting': results['train_r2_mean'] - results['R¬≤_mean'],\n",
    "            'CV Time (s)': cv_times[model_name]\n",
    "        })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_summary)\n",
    "cv_df = cv_df.sort_values('R¬≤ Mean', ascending=False)\n",
    "\n",
    "# Mostrar tabla\n",
    "print(cv_df.round(4).to_string(index=False))\n",
    "\n",
    "# Identificar el mejor modelo seg√∫n CV\n",
    "best_cv_model = cv_df.iloc[0]\n",
    "print(f\"\\nüèÜ MEJOR MODELO SEG√öN CROSS VALIDATION:\")\n",
    "print(f\"  ü•á Modelo: {best_cv_model['Modelo']}\")\n",
    "print(f\"  üìä R¬≤ Promedio: {best_cv_model['R¬≤ Mean']:.4f} ¬± {best_cv_model['R¬≤ Std']:.4f}\")\n",
    "print(f\"  üìä RMSE Promedio: {best_cv_model['RMSE Mean']:.2f} ¬± {best_cv_model['RMSE Std']:.2f}\")\n",
    "print(f\"  üìä Estabilidad: {'Alta' if best_cv_model['R¬≤ Std'] < 0.02 else 'Moderada' if best_cv_model['R¬≤ Std'] < 0.05 else 'Baja'}\")\n",
    "print(f\"  üìä Overfitting: {'Bajo' if best_cv_model['Overfitting'] < 0.05 else 'Moderado' if best_cv_model['Overfitting'] < 0.1 else 'Alto'}\")\n",
    "\n",
    "# An√°lisis de estabilidad\n",
    "print(f\"\\nüìà AN√ÅLISIS DE ESTABILIDAD:\")\n",
    "stable_models = cv_df[cv_df['R¬≤ Std'] < 0.03]['Modelo'].tolist()\n",
    "unstable_models = cv_df[cv_df['R¬≤ Std'] >= 0.05]['Modelo'].tolist()\n",
    "\n",
    "print(f\"  ‚úÖ Modelos Estables (R¬≤ Std < 0.03): {stable_models if stable_models else 'Ninguno'}\")\n",
    "print(f\"  ‚ö†Ô∏è  Modelos Inestables (R¬≤ Std >= 0.05): {unstable_models if unstable_models else 'Ninguno'}\")\n",
    "\n",
    "# An√°lisis de overfitting\n",
    "print(f\"\\nüéØ AN√ÅLISIS DE OVERFITTING:\")\n",
    "good_generalization = cv_df[cv_df['Overfitting'] < 0.05]['Modelo'].tolist()\n",
    "overfitting_models = cv_df[cv_df['Overfitting'] >= 0.1]['Modelo'].tolist()\n",
    "\n",
    "print(f\"  ‚úÖ Buena Generalizaci√≥n (diff < 0.05): {good_generalization if good_generalization else 'Ninguno'}\")\n",
    "print(f\"  ‚ùå Posible Overfitting (diff >= 0.1): {overfitting_models if overfitting_models else 'Ninguno'}\")\n",
    "\n",
    "# Ranking final considerando m√∫ltiples criterios\n",
    "print(f\"\\nüèÖ RANKING FINAL (Cross Validation):\")\n",
    "cv_df['Score_CV'] = (\n",
    "    cv_df['R¬≤ Mean'] * 0.4 +  # 40% peso en R¬≤\n",
    "    (1 - cv_df['R¬≤ Std'] / cv_df['R¬≤ Std'].max()) * 0.3 +  # 30% peso en estabilidad\n",
    "    (1 - cv_df['Overfitting'] / cv_df['Overfitting'].max()) * 0.3  # 30% peso anti-overfitting\n",
    ")\n",
    "\n",
    "cv_df_ranked = cv_df.sort_values('Score_CV', ascending=False)\n",
    "\n",
    "for i, (_, row) in enumerate(cv_df_ranked.head().iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Modelo']} - Score: {row['Score_CV']:.3f}\")\n",
    "    print(f\"     R¬≤: {row['R¬≤ Mean']:.4f}¬±{row['R¬≤ Std']:.4f}, RMSE: {row['RMSE Mean']:.2f}¬±{row['RMSE Std']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones de Cross Validation\n",
    "print(\"\\nüìä VISUALIZACIONES DE CROSS VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar el estilo de las gr√°ficas\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Crear figura con m√∫ltiples subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üìä An√°lisis Cross Validation - Comparaci√≥n de Modelos', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Box plot de R¬≤ scores\n",
    "ax1 = axes[0, 0]\n",
    "r2_data = []\n",
    "model_names = []\n",
    "for model_name, results in cv_results.items():\n",
    "    if results is not None:\n",
    "        r2_data.extend(results['R¬≤_scores'])\n",
    "        model_names.extend([model_name] * len(results['R¬≤_scores']))\n",
    "\n",
    "cv_viz_df = pd.DataFrame({'Model': model_names, 'R¬≤': r2_data})\n",
    "sns.boxplot(data=cv_viz_df, y='Model', x='R¬≤', ax=ax1)\n",
    "ax1.set_title('üìä Distribuci√≥n de R¬≤ por Modelo (5-Fold CV)', fontweight='bold')\n",
    "ax1.set_xlabel('R¬≤ Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot de RMSE scores\n",
    "ax2 = axes[0, 1]\n",
    "rmse_data = []\n",
    "model_names = []\n",
    "for model_name, results in cv_results.items():\n",
    "    if results is not None:\n",
    "        rmse_data.extend(results['RMSE_scores'])\n",
    "        model_names.extend([model_name] * len(results['RMSE_scores']))\n",
    "\n",
    "rmse_viz_df = pd.DataFrame({'Model': model_names, 'RMSE': rmse_data})\n",
    "sns.boxplot(data=rmse_viz_df, y='Model', x='RMSE', ax=ax2)\n",
    "ax2.set_title('üìä Distribuci√≥n de RMSE por Modelo (5-Fold CV)', fontweight='bold')\n",
    "ax2.set_xlabel('RMSE')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Gr√°fico de barras con intervalos de confianza - R¬≤\n",
    "ax3 = axes[1, 0]\n",
    "models_sorted = cv_df.sort_values('R¬≤ Mean', ascending=True)\n",
    "y_pos = np.arange(len(models_sorted))\n",
    "\n",
    "bars = ax3.barh(y_pos, models_sorted['R¬≤ Mean'], \n",
    "                xerr=models_sorted['R¬≤ Std'], \n",
    "                capsize=5, alpha=0.8, color='skyblue', edgecolor='navy')\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(models_sorted['Modelo'])\n",
    "ax3.set_xlabel('R¬≤ Score')\n",
    "ax3.set_title('üìà R¬≤ Promedio ¬± Desviaci√≥n Est√°ndar', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for i, (bar, value, std) in enumerate(zip(bars, models_sorted['R¬≤ Mean'], models_sorted['R¬≤ Std'])):\n",
    "    ax3.text(value + std + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{value:.3f}¬±{std:.3f}', \n",
    "             ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 4. An√°lisis de Overfitting\n",
    "ax4 = axes[1, 1]\n",
    "train_r2 = [cv_results[model]['train_r2_mean'] for model in models_sorted['Modelo'] if cv_results[model] is not None]\n",
    "test_r2 = models_sorted['R¬≤ Mean'].values\n",
    "\n",
    "x_pos = np.arange(len(models_sorted))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x_pos - width/2, train_r2, width, label='Training R¬≤', alpha=0.8, color='lightcoral')\n",
    "bars2 = ax4.bar(x_pos + width/2, test_r2, width, label='Validation R¬≤', alpha=0.8, color='lightblue')\n",
    "\n",
    "ax4.set_xlabel('Modelos')\n",
    "ax4.set_ylabel('R¬≤ Score')\n",
    "ax4.set_title('üéØ An√°lisis de Overfitting (Train vs Validation)', fontweight='bold')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(models_sorted['Modelo'], rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# L√≠nea de referencia\n",
    "ax4.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='Target R¬≤=0.8')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas adicionales\n",
    "print(f\"\\nüìà ESTAD√çSTICAS ADICIONALES:\")\n",
    "print(f\"  üìä Mejor R¬≤ promedio: {cv_df['R¬≤ Mean'].max():.4f} ({cv_df.loc[cv_df['R¬≤ Mean'].idxmax(), 'Modelo']})\")\n",
    "print(f\"  üìä Menor RMSE promedio: {cv_df['RMSE Mean'].min():.2f} ({cv_df.loc[cv_df['RMSE Mean'].idxmin(), 'Modelo']})\")\n",
    "print(f\"  üìä Mayor estabilidad: {cv_df.loc[cv_df['R¬≤ Std'].idxmin(), 'Modelo']} (R¬≤ Std: {cv_df['R¬≤ Std'].min():.4f})\")\n",
    "print(f\"  üìä Menor overfitting: {cv_df.loc[cv_df['Overfitting'].idxmin(), 'Modelo']} (diff: {cv_df['Overfitting'].min():.4f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Cross Validation completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077ea31",
   "metadata": {},
   "source": [
    "### üéØ **Conclusiones del Cross Validation**\n",
    "\n",
    "#### üìä **Principales Hallazgos:**\n",
    "\n",
    "1. **üèÜ Mejor Modelo General:** El modelo que obtuvo el mejor balance entre rendimiento, estabilidad y generalizaci√≥n\n",
    "\n",
    "2. **üìà Estabilidad de Modelos:** \n",
    "   - **Modelos Estables:** Aquellos con baja variabilidad en R¬≤ entre folds\n",
    "   - **Modelos Inestables:** Aquellos que muestran alta variabilidad\n",
    "\n",
    "3. **üéØ An√°lisis de Overfitting:**\n",
    "   - **Buena Generalizaci√≥n:** Modelos con poca diferencia entre train y validation\n",
    "   - **Overfitting Detectado:** Modelos que funcionan mucho mejor en entrenamiento\n",
    "\n",
    "4. **‚ö° Eficiencia Computacional:** Tiempo de entrenamiento vs. rendimiento\n",
    "\n",
    "#### üîç **Recomendaciones:**\n",
    "\n",
    "- **Para Producci√≥n:** Usar el modelo con mejor balance de precisi√≥n, estabilidad y velocidad\n",
    "- **Para Investigaci√≥n:** Considerar los modelos con mayor precisi√≥n aunque sean menos estables\n",
    "- **Para Tiempo Real:** Priorizar modelos r√°pidos con rendimiento aceptable\n",
    "\n",
    "#### üìã **Validaci√≥n Robusta:**\n",
    "El cross validation confirma la robustez de nuestros resultados y proporciona una estimaci√≥n m√°s confiable del rendimiento esperado en datos no vistos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
