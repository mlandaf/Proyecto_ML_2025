# ğŸš´â€â™‚ï¸ PredicciÃ³n de Demanda de Bicicletas Compartidas
## Proyecto de Machine Learning - SecciÃ³n 753, Universidad de Lima

### ï¿½ **Integrantes del Equipo**

| **Integrante** | **Responsabilidad** |
|----------------|-------------------|
| **Renzo Aguilar** | Fase de AnÃ¡lisis de Datos |
| **Franco Melchor y Carlos Minauro** | Fase de Preprocesamiento y Tratamiento de Datos |
| **Marcelo Landa** | Fase de Modelado |
| **Alejandro Toribio** | Fase de OptimizaciÃ³n |

---

### ï¿½ğŸ“‹ **InformaciÃ³n del Proyecto**
- **Curso**: Machine Learning - SecciÃ³n 753
- **Universidad**: Universidad de Lima
- **Dataset**: [Bike Sharing Dataset - UCI ML Repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)
- **Objetivo**: Desarrollar modelos de predicciÃ³n de demanda de bicicletas utilizando algoritmos de machine learning

---

## ğŸ¯ **Objetivo del Proyecto**

Implementar y comparar mÃºltiples algoritmos de machine learning para predecir la demanda de bicicletas compartidas, siguiendo una metodologÃ­a rigurosa que incluye anÃ¡lisis exploratorio, preprocesamiento, modelado y optimizaciÃ³n de hiperparÃ¡metros.

---

## ğŸ“Š **Dataset**

**Fuente**: UCI Machine Learning Repository - Bike Sharing Dataset  
**DescripciÃ³n**: Datos histÃ³ricos de un sistema de bicicletas compartidas con informaciÃ³n meteorolÃ³gica y temporal  
**TamaÃ±o**: 731 observaciones, 16 variables  
**Variable objetivo**: `cnt` (demanda total de bicicletas)

### **Variables principales:**
- **Temporales**: aÃ±o, mes, hora, dÃ­a de la semana, estaciÃ³n
- **MeteorolÃ³gicas**: temperatura, humedad, velocidad del viento, condiciones climÃ¡ticas
- **Contextuales**: dÃ­a laborable, festivo

---

## ğŸ”¬ **MetodologÃ­a Implementada**

### **Algoritmos Utilizados** (segÃºn especificaciones del curso):
1. **Support Vector Machine (SVM)**
2. **Decision Tree**
3. **Random Forest**
4. **Neural Network (MLP)**
5. **XGBoost**
6. **Bagging**
7. **Gradient Boosting**
8. **Voting Regressor**

### **TÃ©cnicas Aplicadas**:
- âœ… **ValidaciÃ³n Cruzada** (K-Fold = 5) para todos los algoritmos
- âœ… **OptimizaciÃ³n de HiperparÃ¡metros** mediante Grid Search
- âœ… **Pipeline** para prevenir data leakage
- âœ… **Escalamiento selectivo** de caracterÃ­sticas
- âœ… **AnÃ¡lisis de overfitting** y generalizaciÃ³n

---

## ğŸ“ **Estructura del Proyecto**

### **Fase 1: AnÃ¡lisis de Datos**
- **ExploraciÃ³n inicial** del dataset
- **AnÃ¡lisis de valores faltantes** (ninguno encontrado)
- **EstadÃ­stica descriptiva** y distribuciones
- **AnÃ¡lisis de correlaciones** entre variables
- **Patrones temporales** y estacionales
- **DetecciÃ³n de outliers**

**Hallazgos clave:**
- Variables mÃ¡s correlacionadas: temperatura (0.63), aÃ±o (0.57), hora (0.40)
- Patrones claros de uso: picos en horas laborales (8AM, 5-6PM)
- Mayor demanda en primavera/verano

### **Fase 2: Preprocesamiento y Tratamiento de Datos**
- **DivisiÃ³n estratificada**: 80% entrenamiento / 20% prueba
- **Escalamiento de caracterÃ­sticas**: aplicado solo a variables numÃ©ricas especÃ­ficas
- **Pipeline implementation**: para prevenir data leakage
- **Feature engineering**: preparaciÃ³n de variables cÃ­clicas

**Resultados:**
- 584 observaciones de entrenamiento
- 147 observaciones de prueba
- No se requiriÃ³ imputaciÃ³n (sin valores faltantes)

### **Fase 3: Modelado**
- **Entrenamiento** de 8 algoritmos diferentes
- **Cross-Validation** con metodologÃ­a Pipeline
- **EvaluaciÃ³n comparativa** usando mÃºltiples mÃ©tricas
- **AnÃ¡lisis de estabilidad** y generalizaciÃ³n

**Resultados obtenidos:**
| Modelo | RÂ² Score | RMSE | Ranking |
|--------|----------|------|---------|
| Gradient Boosting | 0.9403 | 44.27 | 1Â° |
| XGBoost | 0.9396 | 44.56 | 2Â° |
| Bagging | 0.9344 | 46.42 | 3Â° |
| Neural Network | 0.9333 | 46.79 | 4Â° |
| Voting | 0.9253 | 49.54 | 5Â° |

### **Fase 4: OptimizaciÃ³n**
- **Grid Search** para los Top 5 modelos
- **OptimizaciÃ³n de hiperparÃ¡metros** especÃ­ficos por algoritmo
- **ValidaciÃ³n final** del mejor modelo
- **AnÃ¡lisis comparativo** base vs optimizado

**Modelo Final Optimizado:**
- **Algoritmo**: Gradient Boosting
- **RÂ² Score**: 0.9480 (94.8% de varianza explicada)
- **RMSE**: 40.89 bicicletas
- **MAE**: 24.78 bicicletas

---

## ğŸ† **Resultados Principales**

### **Mejor Modelo: Gradient Boosting Optimizado**
```python
HiperparÃ¡metros Ã³ptimos:
- learning_rate: 0.1
- max_depth: 7
- n_estimators: 300
- subsample: 0.9
```

### **MÃ©tricas de Rendimiento:**
- **PrecisiÃ³n**: 94.8% de varianza explicada
- **Error promedio**: ~41 bicicletas (RMSE)
- **Error absoluto**: ~25 bicicletas (MAE)
- **GeneralizaciÃ³n**: Excelente (sin overfitting)

### **Mejoras logradas con optimizaciÃ³n:**
- Gradient Boosting: +0.58% mejora
- XGBoost: +0.64% mejora
- Neural Network: +0.07% mejora

---

## ğŸ› ï¸ **Stack TecnolÃ³gico**

- **Python 3.x**
- **Pandas**: ManipulaciÃ³n de datos
- **NumPy**: Operaciones numÃ©ricas
- **Scikit-learn**: Algoritmos ML y Pipeline
- **XGBoost**: Gradient boosting optimizado
- **Matplotlib/Seaborn**: VisualizaciÃ³n
- **Jupyter Notebook**: Desarrollo interactivo

---

## ğŸ“ˆ **Insights de Negocio**

### **Factores clave de demanda:**
1. **Temperatura**: CorrelaciÃ³n positiva fuerte
2. **Horarios pico**: 8:00 AM y 5:00-6:00 PM
3. **Estacionalidad**: Primavera/verano > otoÃ±o/invierno
4. **DÃ­a laborable**: Patrones diferentes vs fines de semana

### **Aplicaciones prÃ¡cticas:**
- RedistribuciÃ³n predictiva de bicicletas
- PlanificaciÃ³n de mantenimiento
- Estrategias de marketing temporal
- OptimizaciÃ³n de inventario

---

## ğŸ“ **Cumplimiento de Especificaciones AcadÃ©micas**

âœ… **Algoritmos requeridos**: Todos implementados (8/8)  
âœ… **ValidaciÃ³n cruzada**: Aplicada a todos los modelos  
âœ… **OptimizaciÃ³n**: Grid Search para Top 5  
âœ… **Tratamiento de datos**: Pipeline y preprocesamiento  
âœ… **4 Fases**: Completadas segÃºn especificaciones  
âœ… **MetodologÃ­a cientÃ­fica**: Anti-overfitting y validaciÃ³n robusta  

---

## ğŸš€ **Conclusiones**

El proyecto ha logrado desarrollar un **modelo de predicciÃ³n altamente preciso** (RÂ² = 0.948) siguiendo todas las especificaciones del curso. La metodologÃ­a implementada garantiza resultados cientÃ­ficamente vÃ¡lidos y aplicables en entornos reales.

**Logros destacados:**
- Modelo final con 94.8% de precisiÃ³n
- MetodologÃ­a robusta sin data leakage
- ComparaciÃ³n exhaustiva de 8 algoritmos
- OptimizaciÃ³n exitosa de hiperparÃ¡metros
- ValidaciÃ³n rigurosa de resultados

---

*Proyecto desarrollado para el curso de Machine Learning, SecciÃ³n 753, Universidad de Lima - 2025*