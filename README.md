# üö¥‚Äç‚ôÇÔ∏è Predicci√≥n de Demanda de Bicicletas Compartidas
## Proyecto de Machine Learning - Secci√≥n 753, Universidad de Lima

### ÔøΩ **Integrantes del Equipo**

| **Integrante** | **Responsabilidad** |
|----------------|-------------------|
| **Renzo Aguilar** | Fase de An√°lisis de Datos |
| **Franco Melchor y Carlos Minauro** | Fase de Preprocesamiento y Tratamiento de Datos |
| **Marcelo Landa** | Fase de Modelado |
| **Alejandro Toribio** | Fase de Optimizaci√≥n |

---

### ÔøΩüìã **Informaci√≥n del Proyecto**
- **Curso**: Machine Learning - Secci√≥n 753
- **Universidad**: Universidad de Lima
- **Dataset**: [Bike Sharing Dataset - UCI ML Repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)
- **Objetivo**: Desarrollar modelos de predicci√≥n de demanda de bicicletas utilizando algoritmos de machine learning

---

## üéØ **Objetivo del Proyecto**

Implementar y comparar m√∫ltiples algoritmos de machine learning para predecir la demanda de bicicletas compartidas, siguiendo una metodolog√≠a rigurosa que incluye an√°lisis exploratorio, preprocesamiento, modelado y optimizaci√≥n de hiperpar√°metros.

---

## üìä **Dataset**

**Fuente**: UCI Machine Learning Repository - Bike Sharing Dataset  
**Descripci√≥n**: Datos hist√≥ricos de un sistema de bicicletas compartidas con informaci√≥n meteorol√≥gica y temporal  
**Tama√±o**: 731 observaciones, 16 variables  
**Variable objetivo**: `cnt` (demanda total de bicicletas)

### **Variables principales:**
- **Temporales**: a√±o, mes, hora, d√≠a de la semana, estaci√≥n
- **Meteorol√≥gicas**: temperatura, humedad, velocidad del viento, condiciones clim√°ticas
- **Contextuales**: d√≠a laborable, festivo

---

## üî¨ **Metodolog√≠a Implementada**

### **Algoritmos Utilizados** (seg√∫n especificaciones del curso):
1. **Support Vector Machine (SVM)**
2. **Decision Tree**
3. **Random Forest**
4. **Neural Network (MLP)**
5. **XGBoost**
6. **Bagging**
7. **Gradient Boosting**
8. **Voting Regressor**

### **T√©cnicas Aplicadas**:
- ‚úÖ **Validaci√≥n Cruzada** (K-Fold = 5) para todos los algoritmos
- ‚úÖ **Optimizaci√≥n de Hiperpar√°metros** mediante Grid Search
- ‚úÖ **Pipeline** para prevenir data leakage
- ‚úÖ **Escalamiento selectivo** de caracter√≠sticas
- ‚úÖ **An√°lisis de overfitting** y generalizaci√≥n

---

## üìÅ **Estructura del Proyecto**

### **Fase 1: An√°lisis de Datos**
- **Exploraci√≥n inicial** del dataset
- **An√°lisis de valores faltantes** (ninguno encontrado)
- **Estad√≠stica descriptiva** y distribuciones
- **An√°lisis de correlaciones** entre variables
- **Patrones temporales** y estacionales
- **Detecci√≥n de outliers**

**Hallazgos clave:**
- Variables m√°s correlacionadas: temperatura (0.63), a√±o (0.57), hora (0.40)
- Patrones claros de uso: picos en horas laborales (8AM, 5-6PM)
- Mayor demanda en primavera/verano

### **Fase 2: Preprocesamiento y Tratamiento de Datos**
- **Divisi√≥n estratificada**: 80% entrenamiento / 20% prueba
- **Escalamiento de caracter√≠sticas**: aplicado solo a variables num√©ricas espec√≠ficas
- **Pipeline implementation**: para prevenir data leakage
- **Feature engineering**: preparaci√≥n de variables c√≠clicas

**Resultados:**
- 584 observaciones de entrenamiento
- 147 observaciones de prueba
- No se requiri√≥ imputaci√≥n (sin valores faltantes)

### **Fase 3: Modelado**
- **Entrenamiento** de 8 algoritmos diferentes
- **Cross-Validation** con metodolog√≠a Pipeline
- **Evaluaci√≥n comparativa** usando m√∫ltiples m√©tricas
- **An√°lisis de estabilidad** y generalizaci√≥n

**Resultados obtenidos:**
| Modelo | R¬≤ Score | RMSE | Ranking |
|--------|----------|------|---------|
| Gradient Boosting | 0.9403 | 44.27 | 1¬∞ |
| XGBoost | 0.9396 | 44.56 | 2¬∞ |
| Bagging | 0.9344 | 46.42 | 3¬∞ |
| Neural Network | 0.9288 | 48.34 | 4¬∞ |
| Voting | 0.9253 | 49.54 | 5¬∞ |

### **Fase 4: Optimizaci√≥n**
- **Grid Search** para los Top 5 modelos
- **Optimizaci√≥n de hiperpar√°metros** espec√≠ficos por algoritmo
- **Validaci√≥n final** del mejor modelo
- **An√°lisis comparativo** base vs optimizado

**Modelo Final Optimizado:**
- **Algoritmo**: Gradient Boosting
- **R¬≤ Score**: 0.9497 (Cross-Validation) / 0.9480 (Test)
- **RMSE**: 40.89 bicicletas
- **MAE**: 24.78 bicicletas

---

## üèÜ **Resultados Principales**

### **Mejor Modelo: Gradient Boosting Optimizado**
```python
Hiperpar√°metros √≥ptimos:
- learning_rate: 0.1
- max_depth: 7
- n_estimators: 300
- subsample: 0.9
```

### **M√©tricas de Rendimiento:**
- **Precisi√≥n**: 94.8% de varianza explicada
- **Error promedio**: ~41 bicicletas (RMSE)
- **Error absoluto**: ~25 bicicletas (MAE)
- **Generalizaci√≥n**: Excelente (sin overfitting)

### **Mejoras logradas con optimizaci√≥n:**
- Gradient Boosting: +1.00% mejora
- XGBoost: +0.97% mejora
- Neural Network: +0.68% mejora
- Voting: +0.75% mejora
- Bagging: +0.03% mejora

---

## üõ†Ô∏è **Stack Tecnol√≥gico**

- **Python 3.10.16**
- **Pandas**: Manipulaci√≥n de datos
- **NumPy**: Operaciones num√©ricas
- **Scikit-learn**: Algoritmos ML y Pipeline
- **XGBoost**: Gradient boosting optimizado
- **Matplotlib/Seaborn**: Visualizaci√≥n
- **Jupyter Notebook**: Desarrollo interactivo

---

## üìà **Insights de Negocio**

### **Factores clave de demanda:**
1. **Temperatura (`temp`)** - Correlaci√≥n fuerte positiva con demanda
2. **Hora del d√≠a (`hr`)** - Patrones claros de uso (picos matutinos y vespertinos)
3. **Condiciones clim√°ticas (`weathersit`)** - Impacto significativo en la demanda

### **Aplicaciones pr√°cticas:**
- Redistribuci√≥n predictiva de bicicletas
- Planificaci√≥n de mantenimiento
- Estrategias de marketing temporal
- Optimizaci√≥n de inventario

---

## üéì **Cumplimiento de Especificaciones Acad√©micas**

‚úÖ **Algoritmos requeridos**: Todos implementados (8/8)  
‚úÖ **Validaci√≥n cruzada**: Aplicada a todos los modelos  
‚úÖ **Optimizaci√≥n**: Grid Search para Top 5  
‚úÖ **Tratamiento de datos**: Pipeline y preprocesamiento  
‚úÖ **4 Fases**: Completadas seg√∫n especificaciones  
‚úÖ **Metodolog√≠a cient√≠fica**: Anti-overfitting y validaci√≥n robusta  

---

## üöÄ **Conclusiones**

El proyecto ha logrado desarrollar un **modelo de predicci√≥n altamente preciso** (R¬≤ = 0.9480) siguiendo todas las especificaciones del curso. La metodolog√≠a implementada garantiza resultados cient√≠ficamente v√°lidos y aplicables en entornos reales.

**Logros destacados:**
- Modelo final con 94.8% de precisi√≥n
- Metodolog√≠a robusta sin data leakage
- Comparaci√≥n exhaustiva de 8 algoritmos
- Optimizaci√≥n exitosa de hiperpar√°metros
- Validaci√≥n rigurosa de resultados

---

*Proyecto desarrollado para el curso de Machine Learning, Secci√≥n 753, Universidad de Lima - 2025*